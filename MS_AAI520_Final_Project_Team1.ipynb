{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7846f45-e90c-44c2-8ad1-e1f8db4e3325",
   "metadata": {},
   "source": [
    "# UNIVERSITY OF SAN DIEGO - MS AAI\n",
    "## Natural Language Processing and Generative AI\n",
    "### Final Project - Team 1: Multi-Agent Financial Analysis System.\n",
    "#### By Manikandan Perumal & Israel Romero Olvera\n",
    "#### _________________________________________________\n",
    "#### The purpose of this final project is to build a real-world financial analysis system powered by agentic AI, with the abilities of reasoning, planning, and acting based on the user's prompt. It will coordinate multiple specialized LLM agents to handle complex financial tasks end-to-end.\n",
    "#### Our Agentic AI system was developed in a folder structure that can be found in our GitHub site: https://github.com/isralennon/AAI_520_Group_1/tree/main\n",
    "#### For delivery purposes we've condensed all the code into this document, structured the following way:\n",
    "#### **1. Tools** - this section contains the code in file /modules/tools.py which will perform basic RAG connections.\n",
    "#### **2. Parser** - this section contains the code in file /modules/parser.py, which provides basic functionality to parse data in JSON format.\n",
    "#### **3. Memory** - this section contains the code in file /modules/memory.py that handles the storage of ongoing knowledge, to provide a robust and efficient functionality.\n",
    "#### **4. Agents** - this section contains the code in file /modules/subagents.py, designed to host the definitions of the main Agent class as well as our specialized subagents - the team of agents available to the main orchestrator. We developed the following team of agents:\n",
    "#### - **News Researcher** - the specialist of finding financial news, using FinnHub.\n",
    "#### - **Market Researcher** - the specialist of finding financial hard data like market trends, stock prices, etc.\n",
    "#### - **Writer** - the specialist of taking all the information and preparing a polished answer for the user\n",
    "#### **5. Main Orchestrator Agent** - this section contains the code in file /modules/agent.py and has the definition for the orchestrator agent, which develops the strategy and coordinates all subagents.\n",
    "#### **6. Demo** - this section contains the code in our main notebook, /main.ipynb - our implementation file where we execute all the above with demonstration purposes.\n",
    "#### _________________________________________________\n",
    "### 1. TOOLS.\n",
    "#### One of the four agent functions we'll implement is the usage of tools, which will be defined in this first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5847078-4728-443e-9d7f-3a350066678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isral\\.conda\\envs\\Transformers_3_10\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "#import modules.tools as tools\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import finnhub\n",
    "from typing import Callable\n",
    "from datetime import datetime, timedelta\n",
    "from google import genai\n",
    "import openai\n",
    "\n",
    "# For privacy reasons, we'll store our token keys on a .env file, which we'll load here:\n",
    "dotenv.load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# First, we'll define a generic Tool class, which will serve as a structure for all of our tools\n",
    "class Tool:\n",
    "    def __init__(self, name, function, description, api=None): # This is the initialization method of the class\n",
    "        self.name = name # Placeholder for the name of the tool\n",
    "        self.function = function # Placeholder for the code of the tool's function\n",
    "        self.description = description # Placeholder for the description of the tool - very important since the agents will use this description to know what the tool does\n",
    "        self.api = api  # Placeholder for API details when needed\n",
    "        \n",
    "    def to_dict(self): # The structure of each class will always be a standard dictionary object that can be easily interpreted by the Agents\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"api\": self.api\n",
    "        }\n",
    "    \n",
    "    def invoke(self, **kwargs): # This is the placeholder of the function for the tool, which will receive a variable number of parameters\n",
    "        print(f\"Invoking {self.name} with arguments {kwargs}\")\n",
    "        return self.function(**kwargs) # Returning the results of the function\n",
    "\n",
    "# Next, we'll declare each individual tool as a class, inheriting from the generic class Tool above\n",
    "class YahooFinance(Tool): # The first tool is YahooFinance, which will pull stock quotes for a given financial symbol, like AAPL for Apple\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Yahoo Finance Stock Quote\", # Name of the tool\n",
    "            function=self.get_stock_quote_yahoo, # Pointing to the YahooFinance function below as this class's own function\n",
    "            description=\"Get the latest stock quote for a given symbol from Yahoo Finance.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "        )\n",
    "    def get_stock_quote_yahoo(self, symbol: str, step: str='') -> dict: # This is the function that pulls the stock using YahooFinance API\n",
    "        # Here we'll perform the call to YahooFinance to get the data from the specified symbol.\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        # Then, we'll use the 'fast_info' method, which pulls basic financial information, including the price.\n",
    "        try:\n",
    "            info = ticker.fast_info # Pulling the information and parsing it to return it\n",
    "            return {\n",
    "                \"symbol\": symbol,\n",
    "                \"last_price\": info[\"lastPrice\"],\n",
    "                \"day_high\": info[\"dayHigh\"],\n",
    "                \"day_low\": info[\"dayLow\"],\n",
    "                \"previous_close\": info[\"previousClose\"]\n",
    "            }\n",
    "        except Exception as e: # Should there be any errors, we will print the error message instead and return an empty dictionary\n",
    "            print(f\"Yahoo Finance API error: {e}\")\n",
    "            return {}\n",
    "#Now, we'll continue with the class that calls Financial Modeling Prep API\n",
    "class FMP(Tool):\n",
    "    def __init__(self,name:str,function:Callable=None,description:str=None,api:str=None,endPoint:str=None):\n",
    "        super().__init__(name=name,function=self.execute if function==None else function,description=description,api=api)\n",
    "        self.endpoint = endPoint if endPoint!=None else  os.getenv(\"FMP_Endpoint\") # It reads the endpoint from our .env file\n",
    "        self.apikey = os.getenv(\"FMP_API_KEY\") # It also reads the API key from our .env file\n",
    "    def execute(self, symbol: str) -> dict: # This is the function that pulls the stock data using FMP API\n",
    "        params = { #These are the parameters for the API call in a dictionary format\n",
    "            \"symbol\": symbol,\n",
    "            \"apikey\": self.apikey,\n",
    "            \"exchange\": \"NASDAQ\"\n",
    "        }\n",
    "        try: #Then we'll try to make the call to the API and return its formatted response as a JSON text\n",
    "            # print(f'Calling FMP API at endpoint: {self.endpoint} with params: {params}')\n",
    "            response=requests.get(self.endpoint, params=params)\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'FMP API error: {e}')\n",
    "            return {}\n",
    "        \n",
    "class StockQuote(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Stack Quote\", # Name of the tool\n",
    "            description=\"Get the latest stock quote for a given symbol from Stack Quote.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/quote' # It reads the endpoint from our .env file\n",
    "        )\n",
    "\n",
    "        \n",
    "class StockPriceChange(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Stock Price Change\", # Name of the tool\n",
    "            description=\"Get the stock price change for a given symbol over the past.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\", \"days\": 7}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/stock-price-change' # It reads the endpoint from our .env file\n",
    "        )\n",
    "        \n",
    "class IncomeStatement(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Income Statement\", # Name of the tool\n",
    "            description=\"Get the income statement for a given symbol from Financial Modeling Prep (FMP).\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/income-statement' # It reads the endpoint from our .env file\n",
    "        )\n",
    "  \n",
    "class FinancialScore(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Financial Score\", # Name of the tool\n",
    "            description=\"Get the financial score for a given symbol from Financial Modeling Prep (FMP).\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\" ,# Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/financial-scores' # It reads the endpoint from our .env file\n",
    "        )\n",
    "   \n",
    "        \n",
    "#We'll be using FinnHub as our News provider next\n",
    "class FinancialNews(Tool): \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub News\", # Name of the tool\n",
    "            function=self.get_stock_quote_finnhub, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the latest financial news for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when it uses this class\n",
    "        )\n",
    "    def get_stock_quote_finnhub(self, symbol: str, step: str='') -> dict: # This is the function that pulls the news data using FinnHub\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        # Next, we setup the client to perform calls:\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "\n",
    "        # Setting a time frame for the news, ending today and starting a week ago\n",
    "        end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "        start_date = (datetime.today() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Now, we call the API, returning the news in the already pre-formatted dictionary structure.\n",
    "        try:\n",
    "            news= finn_client.company_news(symbol, _from=start_date, to=end_date) # Calling the FINN API\n",
    "            if len(news)==0:\n",
    "                return {\"message\": f\"No news found for symbol {symbol} from {start_date} to {end_date}.\"}\n",
    "            top_news = sorted(news, key=lambda x: x['datetime'], reverse=True)[:5] # Pulls only the top 5 news\n",
    "            top_news_formatted = []\n",
    "            for item in top_news: # Iterates through each news item to pull the headline, summary, and date.\n",
    "                top_news_formatted.append({\n",
    "                    \"headline\": item.get(\"headline\"),\n",
    "                    \"summary\": item.get(\"summary\"),\n",
    "                    \"datetime\": datetime.fromtimestamp(item.get(\"datetime\")).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "        \n",
    "            return { # Returns the formatted news for the specified symbol\n",
    "                \"symbol\": symbol,\n",
    "                \"news\": top_news_formatted  \n",
    "            }\n",
    "    \n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}\n",
    "\n",
    "class RecommendationTrends(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub Recommendation Trends\", # Name of the tool\n",
    "            function=self.get_recommendation_trends, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the recommendation trends for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when this class\n",
    "        )\n",
    "    def get_recommendation_trends(self, symbol: str) -> dict:\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "        try:\n",
    "            return finn_client.recommendation_trends(symbol)\n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}\n",
    "        \n",
    "class EarningSurprise(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub Earning Surprise\", # Name of the tool\n",
    "            function=self.get_earning_surprise, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the earning surprise for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when this class\n",
    "        )\n",
    "    def get_earning_surprise(self, symbol: str) -> dict:\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "        try:\n",
    "            return finn_client.company_earnings(symbol,limit=5)\n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd7b2a-b7e4-45df-8551-153c0d0da84b",
   "metadata": {},
   "source": [
    "### 2. PARSER\n",
    "#### One of the workflow patterns our agents will do is routing, meaning our main agent will coordinate with subagents. To accomplish this communication, we need a \"common language\", which in this case will be JSON. This section defines the functions to implement the JSON parsing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5fdb39-7ca7-4dca-8fb9-6173fa06dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll define first a Parser abstract class\n",
    "class Parser:\n",
    "    def parse(self, response): # This is the placeholder of the default method for this class\n",
    "        # Here's the returned value, which will be a dictionary with an Action value, and a list of dynamic parameters.\n",
    "        return {\"action\": \"FinalAnswer\", \"parameters\": {}}\n",
    "# Next, we'll define an XML parser, which inherits from our abstract class Parser.    \n",
    "class XmlParser(Parser):\n",
    "    def parse(self, response):\n",
    "        # A parser that extracts XML tags from the response.\n",
    "        # For example, it looks for <InvokeTool>{\"symbol\": \"AAPL\", \"step\": \"financials\"}</InvokeTool>\n",
    "        # or <FinalAnswer>answer</FinalAnswer>.\n",
    "        # Returns : a dict with action and parameters. Example:\n",
    "        # {\n",
    "        #    \"action\": \"InvokeTool\",\n",
    "        #    \"parameters\": {\n",
    "        #        \"symbol\": \"AAPL\",\n",
    "        #        \"step\": \"financials\"\n",
    "        #    }\n",
    "        #}\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>' # Defining the regular expression for XML structure\n",
    "        matches = re.findall(pattern, response) # Identifying all matches of XML\n",
    "        if matches: # When there are XML matches, we'll separate them and parse their contents\n",
    "            action, content = matches[0]\n",
    "            content = content.strip()\n",
    "            contentJson = {}\n",
    "            try:\n",
    "                import json\n",
    "                contentJson = json.loads(content) # Once parsed, we'll reformat them to JSON\n",
    "            except:\n",
    "                contentJson = {\"content\": content} # If the content is not valid, we'll return the error message with the invalid content text\n",
    "                return {\"action\": action, \"parameters\": contentJson, \"error\": \"Content is not valid JSON\"}\n",
    "            return {\"action\": action, \"parameters\": contentJson} # If it was valid, we return the parsed content in JSON format\n",
    "        return {\"action\": \"FinalAnswer\", \"parameters\": {}} #If there wasn't any XML to begin with, we just return an empty list of parameters\n",
    "    # Next, we have a specialized parsing for our agent's functionality that will interpret the actions in XML tags and encode them as a list of dictionaries\n",
    "    def parse_all(self, response):\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>' # Defining the regular expression for XML structure\n",
    "        matches = re.findall(pattern, response) # Identifying all matches of XML\n",
    "        results = [] # Preparing an empty array for the results\n",
    "        for action, content in matches: # For each detected action (if any),\n",
    "            content = content.strip()   # we'll parse its contents\n",
    "            contentJson = {}\n",
    "            try: # Then, we'll try to convert it to JSON format\n",
    "                import json\n",
    "                contentJson = json.loads(content)\n",
    "            except: # Should any errors occur, we'll return the error message as part of the response\n",
    "                contentJson = {\"content\": content}\n",
    "                results.append({\"action\": action, \"parameters\": contentJson, \"error\": \"Content is not valid JSON\"})\n",
    "                continue\n",
    "            results.append({\"action\": action, \"parameters\": contentJson}) # If everything's fine, we'll return the parsed JSON content\n",
    "        if not results:\n",
    "            results.append({\"action\": \"FinalAnswer\", \"parameters\": {}}) # If there were no actions, we'll return an empty dictionary\n",
    "        return results  \n",
    "    \n",
    "    def parseTags(self, response):\n",
    "        '''Agent response parser to extract all TAGS.\n",
    "            Returns a dictionary with tag names as keys and tag values as values.\n",
    "        '''\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>'\n",
    "        matches = re.findall(pattern, response)\n",
    "        result = {}\n",
    "        for tag, value in matches:\n",
    "                result[tag.lower()] = value.strip() \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0e958-92da-4165-acd4-d541e967c259",
   "metadata": {},
   "source": [
    "### 3. MEMORY.\n",
    "#### Another feature of our agent is learning, which means the agent must remember information as it gets prompted to refine their answers and keep getting more knowledgeable as it gets used. The functions that perform such learning are defined in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5315f5-f806-4c34-87f7-f4187f6baa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "# We're creating a class called MemorySystem with all the learning functionality\n",
    "class MemorySystem:\n",
    "    # This class stores insights and lessons from previous analyses to improve future runs.\n",
    "    def __init__(self, memory_file='agent_memory.pkl'): # It will store the learned data into the specified file, or the default file name.\n",
    "        self.memory_file = memory_file\n",
    "        self.stock_insights = {}\n",
    "        self.news_insights = {}\n",
    "        self.load_memory()\n",
    "    \n",
    "    def load_memory(self): # Should there be a previous file in existence, it can load it using this function\n",
    "        try:\n",
    "            if os.path.exists(self.memory_file): # It will look for the file name specified in the instance of this class\n",
    "                with open(self.memory_file, 'rb') as f: # If it exists, it will attempt to open it\n",
    "                    memory_data = pickle.load(f) # Then, it will load the data into memory\n",
    "                    self.stock_insights = memory_data.get('stock_insights', {}) # separating stock insights,\n",
    "                    self.news_insights = memory_data.get('news_insights', {}) # market news insights,\n",
    "            else: # Should there be no prior file, it will start fresh\n",
    "                print(\"No memory file found. Starting with empty memory.\")\n",
    "        except Exception as e: # Should there be an error while loading the file, it will start fresh as well\n",
    "            print(f\"Error loading memory: {e}\")\n",
    "            print(\"Starting with empty memory.\")\n",
    "    \n",
    "    def save_memory(self): # This method will save the memory in the file in a structured manner\n",
    "        try:\n",
    "            memory_data = {\n",
    "                'stock_insights': self.stock_insights, # It will save all stock insights currently provided,\n",
    "                'news_insights': self.news_insights # followed by news insights\n",
    "            }\n",
    "            with open(self.memory_file, 'wb') as f: # It will first open the file name specified in the instance of this class\n",
    "                pickle.dump(memory_data, f) # and then write in it the contents of the memory_data dictionary\n",
    "            print(\"Memory saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving memory: {e}\") # Should there be any errors saving, it will print out the error\n",
    "    \n",
    "    def add_stock_insight(self, symbol, insight, timestamp=None): # With this method, we'll add knowledge classified as stock insights\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().isoformat() # If no timestamp is specified, we'll initialize the current time stamp\n",
    "        \n",
    "        if symbol not in self.stock_insights: # If the current symbol (financial company) is not in previous insights, we'll add it\n",
    "            self.stock_insights[symbol] = []\n",
    "        \n",
    "        self.stock_insights[symbol].append({ # Finally, we encode the insight with its timestamp in the stock_insights dictionary of this class\n",
    "            'insight': insight,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        self.save_memory() # And we save the memory right away\n",
    "    \n",
    "    def add_market_news(self,symbol, news_item, timestamp=None): # This method adds market news insights for a given symbol\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().isoformat() # If no timestamp is specified, we'll initialize the current time stamp\n",
    "\n",
    "        if symbol not in self.news_insights: # If the current symbol (financial company) is not in previous insights, we'll add it\n",
    "            self.news_insights[symbol] = []\n",
    "\n",
    "        self.news_insights[symbol].append({ # Finally, we encode the news item with its timestamp in the news_insights dictionary of this class\n",
    "            'news_item': news_item,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        self.save_memory() # And we save the memory right away\n",
    "\n",
    "    def get_stock_insights(self, symbol): # This method retrieves all stock insights for a given symbol\n",
    "        results=self.stock_insights.get(symbol, [])\n",
    "        if not results:\n",
    "            print(f\"No insights found for symbol {symbol}.\")\n",
    "            return []\n",
    "        if results:\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                # if the timestamp is older than 7 days, we can choose to ignore it\n",
    "                timestamp = datetime.fromisoformat(result['timestamp'])\n",
    "                if (datetime.now() - timestamp).days > 7:\n",
    "                    continue\n",
    "                filtered_results.append(result)\n",
    "        return filtered_results\n",
    "\n",
    "    def get_news_insights(self, symbol): # This method retrieves all market news insights for a given symbol\n",
    "        results=self.news_insights.get(symbol, [])\n",
    "        if not results:\n",
    "            print(f\"No news insights found for symbol {symbol}.\")\n",
    "            return []\n",
    "        if results:\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                # if the timestamp is older than 2 days, we can choose to ignore it\n",
    "                timestamp = datetime.fromisoformat(result['timestamp'])\n",
    "                if (datetime.now() - timestamp).days > 2:\n",
    "                    continue\n",
    "                filtered_results.append(result)\n",
    "        return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf287589-30f8-4c56-a61c-175eeaf9af11",
   "metadata": {},
   "source": [
    "### 4. AGENTS\n",
    "#### For our routing workflow, along with communication also comes specialization and tool usage: a team of agents that will collaborate, coordinated by the main orchestrator agent. That's what we'll define in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13a923fc-91cd-40c0-aeb6-55eb20d0bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we'll initialize the Google GenAI and OpenAI\n",
    "from google import genai\n",
    "import openai\n",
    "#Make sure to load the environmental variables\n",
    "dotenv.load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "class Agent: # This will be our base class for all our agents\n",
    "    def __init__(self, name, role, system_prompt, model, generate_response, agents=None, tools=None, memory_system=None, parser=None, debug=0): # This is the initialization method of the Agent class\n",
    "        self.name = name # Placeholder for the name of the tool\n",
    "        self.model = model # Placeholder for the LLM model\n",
    "        self.role = role # Placeholder for the role of this agent\n",
    "        self.system_prompt = system_prompt # Placeholder for the system prompt that defines this agent\n",
    "        self.memory_system = memory_system # Placeholder for the memory object for this agent - it could be None, so the agent would start without knowledge\n",
    "        self.parser = parser  # Placeholder for API details when needed\n",
    "        self.generate_response = generate_response # Placeholder for the generate response method\n",
    "        self.agents = agents \n",
    "        self.tools = tools # Placeholder for the tools passed on to this agent, which should be a list\n",
    "        self.conversation_history = [] # Initializing a blank conversation history\n",
    "        self.max_history_length = 10 # Initializing a default max number of history length\n",
    "\n",
    "        self.prompt_template = (\n",
    "            \"You are {agent_name}, an AI agent. Use the following tools as needed:\\n\"\n",
    "            \"{tools}\\n\"\n",
    "            \"Conversation history:\\n\"\n",
    "            \"{history}\\n\"\n",
    "            \"Current input: {input}\\n\"\n",
    "            \"Respond appropriately.\"\n",
    "        )\n",
    "        self.initialize_client() #Initializing the LLM client\n",
    "        self.debug = debug #Setting the debug local variable, used to print certain validation statements when set to 1\n",
    "    #We want our Agent class to support multiple LLMs, so this function will help initialize its internal client dynamically.\n",
    "    def initialize_client(self):\n",
    "        #For GPT models\n",
    "        if \"gpt\" in self.model.lower(): \n",
    "            self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        #For Gemini models\n",
    "        elif \"gemini\" in self.model.lower():\n",
    "            self.client = genai.Client()\n",
    "    def to_dict(self): # The structure of each class will always be a standard dictionary object that can be easily interpreted by the Agents\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"api\": self.api\n",
    "        }\n",
    "    def register_tool(self, tool): #This function helps register tools that the agent will have access to.\n",
    "        self.tools.append(tool) \n",
    "    def remember(self, message): #This function enables the agent to remember a message in its conversation history\n",
    "        self.conversation_history.append(message)\n",
    "        if len(self.conversation_history) > self.max_history_length:\n",
    "            self.conversation_history.pop(0) # If the number of elements in the history exceeds the established limit, we'll pop the oldest one\n",
    "    def call_llm(self, input_prompt): #This is the generic call to LLM that agents can use. They may have a different version if needs are unique\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": input_prompt}\n",
    "                    ],\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                prompt = self.system_prompt\n",
    "                prompt += \"\\n\" + input_prompt\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if self.debug == 1:\n",
    "                print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "    def generate_response(self, **kwargs): # This is the placeholder of the generative function for the agent, which will receive a variable number of parameters\n",
    "        if self.debug == 1:\n",
    "            print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        return self.generate_response(**kwargs) # Returning the results of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed17b1-9690-4054-8131-025b22fe39fb",
   "metadata": {},
   "source": [
    "#### Next, let's define some Sub-agents that will inherit from the class above.\n",
    "#### We'll start with the Market Research agent, capable of finding hard-financial data like stock quotes or market trends using some of the financial tools declared at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b97db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketResearchAgent(Agent):\n",
    "    def __init__(self, model=\"gemini-2.5-flash\", debug=0):\n",
    "        name=\"Market Research Agent\"\n",
    "        model=model\n",
    "        role=\"Market Research Agent specialized in financial data analysis and market trends\"\n",
    "        system_prompt=f\"\"\"You are a Market Research Agent specialized in financial data analysis and market trends.\n",
    "         Your role is to assist users by providing accurate and up-to-date financial information, stock quotes, market trends, and insights based on the latest data available from various financial APIs and tools.\n",
    "\n",
    "         Based on the data retrieved from the tools at your disposal, provide comprehensive answers to user queries related to stock performance, market analysis, and financial news.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.memory_system=MemorySystem()\n",
    "        super().__init__(name=name,system_prompt=system_prompt,model=model,generate_response=self.generate_response,role=role,agents=None,tools=None,memory_system=self.memory_system,parser=None, debug=debug) \n",
    "    \n",
    "    def generate_response(self, **kwargs): # This is the generative function for the agent, which receives a prompt and returns the result from the LLM\n",
    "        if self.debug == 1:\n",
    "            print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        input_prompt=kwargs.get(\"prompt\",[])\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": input_prompt}\n",
    "                    ],\n",
    "                    #messages=input_prompt,\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if self.debug == 1:\n",
    "                print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "\n",
    "    def getMarketSummary(self,symbol:str  ) -> str: # This function pulls market data for the specified financial symbol\n",
    "        prompt=f\"\"\"Provide a comprehensive market summary for the stock symbol: {symbol}. \n",
    "                Include recent performance, key financial metrics, and any notable news or trends affecting the stock.\n",
    "                Use data from Yahoo Finance, Financial Modeling Prep, and FinnHub to inform your summary.\n",
    "                Format the response in a clear and concise manner suitable for a financial report.\"\"\"\n",
    "        insights = self.memory_system.get_stock_insights(symbol) # It tries to pull existing insights in memory for the specified symbol\n",
    "        if insights:\n",
    "            if self.debug == 1:\n",
    "                print(f\"Using cached insight for symbol {symbol}.\")\n",
    "            return insights[-1]['insight'] # If found, it will return those cached insights for faster response\n",
    "        else: # If no insights are found for this symbol, the agent will use RAG functions to pull new insights\n",
    "            tools_list=[FinancialScore(),IncomeStatement(),StockQuote(),StockPriceChange()] # Registering available RAG tools for this agent\n",
    "            for tool in tools_list:\n",
    "                tool_response=tool.invoke(symbol=symbol) # We call each RAG tool to pull data for the specified symbol\n",
    "                prompt+=f\"\\nData from {tool.name}: {tool_response}\" # We prepare the prompt with the data returned from the RAG tool\n",
    "            response=self.generate_response(prompt=prompt) # We call the the LLM with the prompts from all tools to produce a response\n",
    "            self.memory_system.add_stock_insight(symbol, response,timestamp=datetime.now().isoformat()) # Adding the insight to the memory system\n",
    "        return response # Returning the LLM response\n",
    "    \n",
    "    def  processUserInput(self, user_input: str) -> str:\n",
    "        tags=self.getEntities(user_input=user_input) # We'll attempt to get the financial symbols from the user's input (i.e. AAPL for Apple, etc)\n",
    "        if \"symbol\" in tags:\n",
    "            marketSummary=self.getMarketSummary(symbol=tags.get(\"symbol\")) # Then, we'll pull financial data for each symbol\n",
    "        prompt=f\"\"\"Based on the {marketSummary} Analyze the following user input\n",
    "                and provide a short answer for the user query.\n",
    "                Rules:\n",
    "                - If the user input is related to stock performance, provide insights based on the market summary.\n",
    "                - If the user input is unrelated to financial markets, respond with \"I'm sorry, I can only assist with financial market-related queries.\"\n",
    "                - Keep the response concise and relevant to the user's query.\n",
    "                - Use a professional and informative tone suitable for financial discussions.\n",
    "                - Limit the response to 150 words.\n",
    "\n",
    "                User Input: \"{user_input}\"\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\",\n",
    "        response=self.generate_response(prompt=prompt) # Passing the financial date to the LLM to generate a response\n",
    "        return response\n",
    "\n",
    "    def getEntities(self, user_input: str) -> str: # This function reviews the user input and pulls any Financial symbols for the mendioned companies\n",
    "        prompt=f\"\"\"Determine entities the following user input related to financial markets and stock analysis:\n",
    "                if the input contains Apple Inc, return SYMBOL as AAPL\n",
    "                if the input contains Microsoft Corporation, return SYMBOL as MSFT\n",
    "                User Input: \"{user_input}\n",
    "                Extracted Entities:\n",
    "                    <SYMBOL>...</SYMBOL>\n",
    "                    <EXCHANGE>...</EXCHANGE><INDUSTRY>...</INDUSTRY>  \"\"\"\n",
    "        response=self.generate_response(prompt=prompt) # Sends the prompt to the LLM to extract the symbols\n",
    "        if self.debug == 1:\n",
    "            print(f'Response: {response}')\n",
    "        parser=XmlParser() # We use our XML parser to extract the symbols from the LLM response\n",
    "        parsed_response=parser.parseTags(response)\n",
    "        return parsed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba108b-b298-4412-a752-a17970a6d742",
   "metadata": {},
   "source": [
    "#### Then, we'll define a Market Sentiment Agent, which will pull financial news using some of the financial tools above, and classify their sentiment, which will be helpful for the research and analsys of the company in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb24a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketSentimentAgent(Agent):\n",
    "    def __init__(self, model=\"gemini-2.5-flash\", debug=0):\n",
    "        name=\"Market News Sentiment Agent\"\n",
    "        model=model\n",
    "        role=\"Market News Sentiment Agent specialized in financial news sentiment analysis\"\n",
    "        system_prompt=f\"\"\"You are a Market Sentiment Agent specialized in financial news sentiment analysis.\n",
    "         Your role is to assist users by analyzing the sentiment of financial news articles and providing insights based on the emotional tone of the content.\n",
    "\n",
    "         Based on the news data retrieved from FinnHub, provide comprehensive sentiment analysis to help users understand market mood and potential impacts on stock performance.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.memory_system=MemorySystem()\n",
    "        super().__init__(name=name,system_prompt=system_prompt,model=model,generate_response=self.generate_response,role=role,agents=None,tools=None,memory_system=self.memory_system,parser=None, debug=debug)\n",
    "        \n",
    "    def generate_response(self, **kwargs): # This is the placeholder of the generative function for the agent, which will receive a variable number of parameters\n",
    "        if self.debug==1:\n",
    "            print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        input_prompt=kwargs.get(\"prompt\",[])\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=input_prompt,\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            return result # Returning the response provided by the selected model\n",
    "        except Exception as e:\n",
    "            if self.debug==1:\n",
    "                print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "        \n",
    "    def getNewsSummary(self,symbol:str  ) -> str:\n",
    "            prompt=f\"\"\"Provide a comprehensive news summary for the stock symbol: {symbol}.\n",
    "                    Include recent news articles, key events, and any notable trends affecting the stock.\n",
    "                    Use data from FinnHub and other news sources to inform your summary.\n",
    "                    Format the response in a clear and concise manner suitable for a financial report.\"\"\"\n",
    "            insights = self.memory_system.get_news_insights(symbol) # If there are any insights found in memory, we'll pull them here\n",
    "            if insights:\n",
    "                if self.debug==1:\n",
    "                    print(f\"Using cached insight for symbol {symbol}.\")\n",
    "                return insights[-1]['news_item'] # We'll return the insights if found.\n",
    "            else: # If there are no insights, we'll call RAG tools to get new insights.\n",
    "                tools_list=[FinancialNews(),RecommendationTrends(),EarningSurprise()] # Declaring the list of financial tools\n",
    "                for tool in tools_list: # Pulling insights from each available tool\n",
    "                    tool_response=tool.invoke(symbol=symbol)\n",
    "                    prompt+=f\"\\nData from {tool.name}: {tool_response}\"\n",
    "                response=self.generate_response(prompt=prompt) # Pass the retrieved RAG data from tools to the LLM to generate a response\n",
    "                self.memory_system.add_market_news(symbol, response,timestamp=datetime.now().isoformat()) # Storing the response in memory\n",
    "            return response\n",
    "        \n",
    "    def processUserInput(self, user_input: str) -> str:\n",
    "        if self.debug==1:\n",
    "            print(\"-\" * 50)\n",
    "            print(f'{self.name}\" received input: {user_input}')\n",
    "            print(\"-\" * 50)\n",
    "        tags=self.getEntities(user_input=user_input) # We'll attempt to get the financial symbols from the user's input (i.e. AAPL for Apple, etc)\n",
    "        if \"symbol\" in tags:\n",
    "            newsSummary=self.getNewsSummary(symbol=tags.get(\"symbol\")) # Then, we'll pull news for each symbol\n",
    "        prompt=f\"\"\"Based on the {newsSummary} Analyze the following user input\n",
    "                and provide a short answer for the user query.\n",
    "                Rules:\n",
    "                - If the user input is related to financial news sentiment, provide insights based on the news summary.\n",
    "                - If the user input is unrelated to financial markets, respond with \"I'm sorry, I can only assist with financial market-related queries.\"\n",
    "                - Keep the response concise and relevant to the user's query.\n",
    "                - Use a professional and informative tone suitable for financial discussions.\n",
    "                - Limit the response to 150 words.\n",
    "\n",
    "                User Input: \"{user_input}\"\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\",\n",
    "        response=self.generate_response(prompt=prompt) # Passing these news to the LLM to generate a response\n",
    "        return response\n",
    "    def getEntities(self, user_input: str) -> str: # This function reviews the user input and pulls any Financial symbols for the mendioned companies\n",
    "        prompt=f\"\"\"Determine entities the following user input related to financial markets and stock analysis:\n",
    "                if the input contains Apple Inc, return SYMBOL as AAPL\n",
    "                if the input contains Microsoft Corporation, return SYMBOL as MSFT\n",
    "                User Input: \"{user_input}\n",
    "                Extracted Entities:\n",
    "                    <SYMBOL>...</SYMBOL>\n",
    "                    <EXCHANGE>...</EXCHANGE><INDUSTRY>...</INDUSTRY>  \"\"\"\n",
    "        response=self.generate_response(prompt=prompt) # Sends the prompt to the LLM to extract the symbols\n",
    "        if self.debug == 1:\n",
    "            print(f'Response: {response}')\n",
    "        parser=XmlParser() # We use our XML parser to extract the symbols from the LLM response\n",
    "        parsed_response=parser.parseTags(response) \n",
    "        return parsed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d0b9f-ecd8-4d6c-a103-acd402e022ef",
   "metadata": {},
   "source": [
    "#### Then, we'll define a Writer agent in charge of providing a polished and structured answer with the researched data provided by the other agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac90380-12c0-4734-8642-c0c0445c230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriterAgent(Agent):\n",
    "    # This agent takes the results of other agents (like news or market research) and creates a professional report that will be returned to the Orchestrator for the Final Response to the user.\n",
    "    def __init__ (self, model=\"gpt-3.5-turbo\", agents=None, memory_system=None, debug=0):\n",
    "        super().__init__(\n",
    "            name=\"Writer\", #Name of the Writer class\n",
    "            role=\"Writer Agent specialized in polished Financial Content\", #Role of this class\n",
    "            system_prompt=(\n",
    "                \"You are Writer, an AI agent part of an agents team. Your role is a professional \"\n",
    "                \"financial report writer, capable of taking financial news \"\n",
    "                \"or financial information provided by the Orchestrator and \"\n",
    "                \"preparing a 23 paragraph report that provides a clear final \"\n",
    "                \"answer to the user.\"\n",
    "                \"Here are some guidelines for you:\"\n",
    "                \"Start your answers giving a positive message like 'Great question', 'Excellent question', or similar.\"\n",
    "                \"Focus on answering the user's question.\"\n",
    "                \"When recommendations are requested, only provide guidance and highlight pros and cons.\"\n",
    "                \"The news or financial information you're receiving came from other agents in the team, so never refer to it as 'the data provided'.\"\n",
    "            ),            \n",
    "            model = model,\n",
    "            generate_response = self.generate_response,\n",
    "            memory_system = memory_system,\n",
    "            debug=debug\n",
    "        )\n",
    "    def generate_response(self, input_prompt): # Calling the default LLM function from the base class\n",
    "        result = self.call_llm(input_prompt)\n",
    "        return result\n",
    "    def processUserInput(self, input_prompt: str) -> str: # This function receives the request from the orchestrator and returns an LLM result.\n",
    "        prompt=input_prompt\n",
    "        response=self.generate_response(input_prompt=prompt)\n",
    "        return response    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d208600-b43d-424f-b54d-7ee6f97c1257",
   "metadata": {},
   "source": [
    "### 5. MAIN ORCHESTRATOR AGENT.\n",
    "#### This is the section where we define the orchestrator agent, which performs the interpretation of the user's prompt, prepares a plan, calls the subagents as needed, and prepares the final answer to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fa6bcc-d542-4354-80bc-95ad0e9cf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorAgent(Agent):\n",
    "    def __init__(self, model, agents=None, memory=None, parser=None, debug=0):\n",
    "        self.agents = agents\n",
    "        self.agents_description = \"\"\n",
    "        for agent in self.agents:\n",
    "            self.agents_description += f'\\n- {agent.name}: {agent.role}'\n",
    "\n",
    "        system_prompt = f'''\n",
    "          You are the leading AI agent for the following team of agents:\n",
    "            {self.agents_description}\n",
    "\n",
    "            You do not generate a response directly to the user, but instead you'll coordinate the agents team by generating a list of tasks for them to do following the Agent Usage guidelines.\n",
    "            \n",
    "            ### Agent Usage Guidelines:\n",
    "\n",
    "            1. Do not respond to the current input directly. Instead, create a plan to call the research agents in your team to pull the necessary data.\n",
    "            2. Convert that plan into a list of calls for your specialized agents (except for the Writer Agent) using an XML structure with the tag \"<SpecializedAgent>\" in the following format:\n",
    "            <SpecializedAgent>{{\"agentName\": \"Market Research Agent\", \"user_input\": \"Your specific query here\"}}</SpecializedAgent>\n",
    "            3. The writer agent will be called separately to finalize the response. Exclude from your thinking process.\n",
    "            \n",
    "            ### Other TAGs you can include in your plan:\n",
    "            -  For thinking, you must wrap your thoughts in <Thought> and </Thought> tags.\n",
    "            -  For final answers, you must wrap your answer in <FinalAnswer> and </FinalAnswer> tags.\n",
    "            -  If you need users to provide more information, you must wrap your request in <RequestMoreInfo> and </RequestMoreInfo> tags.\n",
    "           \n",
    "            ### Instructions for using the tools:\n",
    "            You should only use the information returned by the Agents listed above, never try to get information independently.\n",
    "        '''\n",
    "        system_prompt = system_prompt.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        # We also need to declare a parser\n",
    "        if parser==None:\n",
    "            parser = XmlParser()\n",
    "\n",
    "        super().__init__(\n",
    "            name = \"Orchestrator Agent\", #Name of the Orchestrator class\n",
    "            role = \"Orchestrator Agent that manages tool usage and conversation flow\",\n",
    "            system_prompt = system_prompt,\n",
    "            model = model,\n",
    "            generate_response = self.generate_response,\n",
    "            memory_system = memory,\n",
    "            agents = agents,\n",
    "            parser = parser,\n",
    "            debug = debug # Storing the variable debug, used for printing messages when set to 1\n",
    "        )\n",
    "        self.conversation_history = []\n",
    "        self.prompt_template = (\n",
    "            f\"{self.system_prompt}\\n\"\n",
    "            \"Conversation history:\\n\"\n",
    "            \"{history}\\n\"\n",
    "            \"Current input: {input}\\n\"\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.initialize_client()\n",
    "\n",
    "    def remember(self, message): # This function will keep track of the message in the conversation history\n",
    "        self.conversation_history.append(message)\n",
    "        if len(self.conversation_history) > self.max_history_length:\n",
    "            self.conversation_history.pop(0) #It will also remove the last element if the history exceeds the max length\n",
    "    \n",
    "    def generate_response(self, input_prompt): # This function calls the LLM with the specified prompt \n",
    "        history_text = \"\\n\".join(self.conversation_history)\n",
    "        response = \"\"\n",
    "        prompt = self.prompt_template.format(\n",
    "            history=history_text,\n",
    "            input=input_prompt\n",
    "        )\n",
    "        if self.debug==1:\n",
    "            print(f\"Orchestrator Prompt: {prompt}\")\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    #messages=input_prompt,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            self.remember(f\"User: {input_prompt}\")\n",
    "            self.remember(f\"{self.name}: {response}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if self.debug==1:\n",
    "                print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "        return response\n",
    "    \n",
    "\n",
    "    def get_specialist_opinion(self, agentName, user_input):\n",
    "        # Agent Orchestrator can call other agents to get their opinion on specific user inputs.\n",
    "        MyAgentsTeam = {MyMarketResearcher, MyNewsResearcher, MyWriter}\n",
    "        for agent in self.agents:\n",
    "            if agent.name == agentName: # When we find the agent referred in the parameters,\n",
    "                return agent.processUserInput(user_input) # we  proceed sending the request to that agent\n",
    "        return f\"Agent {agentName} not found.\" # If we reach this line, it means the agent in the parameters wasn't found.\n",
    "        \n",
    "    \n",
    "    def reAct(self, user_input:str)-> str:\n",
    "        # Here is the the logic to parse the response for Agents usage\n",
    "        # and store the results.\n",
    "        parsed_response = \"\"\n",
    "        #Preparing a temporary repository for agent responses\n",
    "        temp_agent_response = \"\"\n",
    "        #We also initialize the content variable we'll pass to the writer\n",
    "        content_for_writer = f'Current user prompt: {user_input}'\n",
    "        if self.parser and self.agents:\n",
    "            response = self.generate_response(user_input) # We ask the LLM to pur together the plan of actions, and store it raw in the response variable.\n",
    "            parsed_response = self.parser.parse_all(response) ## parsed response is a dict {\"InvokeTool\": \"tool_name\", \"parameters\": {...}} or {\"FinalAnswer\": \"answer\"} or {\"RequestMoreInfo\": \"info\"}\n",
    "            if self.debug==1: # If debugging, we'll print the above variables\n",
    "                print(\"*\" * 50)\n",
    "                print(f'Raw actions from Orchestrator: {response}')\n",
    "                print(\"*\" * 50)\n",
    "                print(\"*\" * 50)\n",
    "                print(f'Actions list from Orchestrator: {parsed_response}')\n",
    "                print(\"*\" * 50)\n",
    "            system_message = f\"System: {response}\"\n",
    "            self.remember(system_message) #We store the raw plan in the conversation history\n",
    "            self.conversation_history.append(system_message)\n",
    "            '''\n",
    "            parsed_response= {\n",
    "            \"action\": \"InvokeTool\",\n",
    "            \"parameters\": {\n",
    "                \"symbol\": \"AAPL\",\n",
    "                \"step\": \"financials\"\n",
    "                }\n",
    "            }\n",
    "            '''\n",
    "            # Next, we'll loop through all the actions in the plan to execute one at a time.\n",
    "            for plan_item in parsed_response:\n",
    "                action = plan_item.get(\"action\") # We get the next action on the list\n",
    "                if self.debug==1:\n",
    "                    print(f\"Orchestrator Action: {action}\")\n",
    "                if action == \"SpecializedAgent\": # When the action is to call another agent, we'll prepare the prompt for that agent here\n",
    "                    agent_name = plan_item[\"parameters\"].get(\"agentName\") # Selecting the agent name\n",
    "                    user_input_for_agent = plan_item[\"parameters\"].get(\"user_input\") # Getting the prompt in the plan for that agent\n",
    "                    if self.debug==1:\n",
    "                        print(\"-\" * 50)\n",
    "                        print(f'Orchestrator calling {agent_name} with prompt \"{user_input_for_agent}\"')\n",
    "                        print(\"-\" * 50)\n",
    "                    agent_response = self.get_specialist_opinion(agent_name, user_input_for_agent) # Getting the response from that agent\n",
    "                    temp_agent_response = f\"Agent {agent_name} Response: {agent_response}\" # Preparing the prompt for the Writer agent by storing each agent's response\n",
    "                    self.remember(temp_agent_response) # Adding each agent's response to memory\n",
    "                    self.conversation_history.append(temp_agent_response) \n",
    "                    content_for_writer += f'\\n\\n{temp_agent_response}' # Attaching each agent's response to the content for the Writer agent\n",
    "                elif action == \"FinalAnswer\" or action == \"RequestMoreInfo\" or action == \"NeedApproval\": # If it's any of these options, we need user input\n",
    "                    print(f\"Orchestrator Final Response: {response}\")\n",
    "                    return parsed_response.get(\"content\")\n",
    "                elif action == \"Thought\": # If it's a thought, we can ignore it on this loop\n",
    "                    continue\n",
    "                else: # Any other actions would be an error.\n",
    "                    return f\"I'm not sure how to proceed. Could you please clarify? - selected action: {action}\"\n",
    "            #Once the loop of actions is completed, we'll pass the information gathered by all research agents down to our writer\n",
    "            response = self.get_specialist_opinion('Writer', content_for_writer) # Then, we'll return the response from the Writer as the Final Answer\n",
    "        else:\n",
    "            parsed_response = \"Error: no parser or sub agents found!\"\n",
    "            print('Parser:')\n",
    "            print(self.parser)\n",
    "            print('Agents:')\n",
    "            print(self.agents)\n",
    "            response = parsed_response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea2e1e-d811-44cb-83eb-1afd0d66f720",
   "metadata": {},
   "source": [
    "### 6. DEMO.\n",
    "#### This is the final section, which contains the implementation of the entire system using all elements above for a quick demonstration.\n",
    "#### We'll start by declaring our team of agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7fae3d-16cf-4e77-9fd0-40f33e0ad6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use \"gpt-3.5-turbo\" or \"gemini-2.5-flash\", #Uncomment if using Google GenAI\n",
    "\n",
    "#Let's declare our sub agent instances first.\n",
    "MyMarketResearcher = MarketResearchAgent(model=\"gemini-2.5-flash\")\n",
    "MyNewsResearcher = MarketSentimentAgent(model=\"gemini-2.5-flash\")\n",
    "MyWriter = WriterAgent(model=\"gemini-2.5-flash\")\n",
    "\n",
    "#We put all of our sub agents together as a list of objects\n",
    "MyAgentsTeam = {MyMarketResearcher, MyNewsResearcher, MyWriter}\n",
    "\n",
    "#Now, we declare our main orchestrator agent instance.\n",
    "MyOrchestrator = OrchestratorAgent(model=\"gpt-3.5-turbo\", agents=MyAgentsTeam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9107463-5820-4b13-9ee0-b63bf8446523",
   "metadata": {},
   "source": [
    "#### Next, we'll define a visual interface to give the tool a \"chatbot\" look and feel. We decided to call it <b>\"CapitalMind Financial Assistant\"</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1958deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for beautiful HTML display in Jupyter notebook\n",
    "from IPython.display import HTML, display, clear_output\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f8d98-e360-4085-a8fc-81a186eab752",
   "metadata": {},
   "source": [
    "#### Now we'll define the visual style of the chat interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39c6eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS styles for beautiful chatbot interface\n",
    "chatbot_css = \"\"\"\n",
    "<style>\n",
    ".chat-container {\n",
    "    max-width: 800px;\n",
    "    margin: 0 auto;\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "    border-radius: 20px;\n",
    "    padding: 20px;\n",
    "    box-shadow: 0 10px 30px rgba(0,0,0,0.3);\n",
    "}\n",
    "\n",
    ".chat-header {\n",
    "    text-align: center;\n",
    "    color: white;\n",
    "    font-size: 24px;\n",
    "    font-weight: bold;\n",
    "    margin-bottom: 20px;\n",
    "    text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\n",
    "}\n",
    "\n",
    ".message {\n",
    "    margin: 15px 0;\n",
    "    display: flex;\n",
    "    align-items: flex-start;\n",
    "}\n",
    "\n",
    ".user-message {\n",
    "    justify-content: flex-end;\n",
    "}\n",
    "\n",
    ".bot-message {\n",
    "    justify-content: flex-start;\n",
    "}\n",
    "\n",
    ".message-bubble {\n",
    "    max-width: 70%;\n",
    "    padding: 15px 20px;\n",
    "    border-radius: 20px;\n",
    "    box-shadow: 0 4px 12px rgba(0,0,0,0.15);\n",
    "    position: relative;\n",
    "    line-height: 1.4;\n",
    "}\n",
    "\n",
    ".user-bubble {\n",
    "    background: linear-gradient(135deg, #36d1dc 0%, #5b86e5 100%);\n",
    "    color: white;\n",
    "    margin-left: 30px;\n",
    "}\n",
    "\n",
    ".bot-bubble {\n",
    "    background: white;\n",
    "    color: #333;\n",
    "    margin-right: 30px;\n",
    "    border: 1px solid #e0e0e0;\n",
    "}\n",
    "\n",
    ".message-avatar {\n",
    "    width: 40px;\n",
    "    height: 40px;\n",
    "    border-radius: 50%;\n",
    "    margin: 0 10px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    font-weight: bold;\n",
    "    color: white;\n",
    "    font-size: 16px;\n",
    "}\n",
    "\n",
    ".user-avatar {\n",
    "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "}\n",
    "\n",
    ".bot-avatar {\n",
    "    background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
    "}\n",
    "\n",
    ".timestamp {\n",
    "    font-size: 11px;\n",
    "    color: rgba(255,255,255,0.7);\n",
    "    margin-top: 5px;\n",
    "    text-align: right;\n",
    "}\n",
    "\n",
    ".bot-timestamp {\n",
    "    color: #999;\n",
    "    text-align: left;\n",
    "}\n",
    "\n",
    ".typing-indicator {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: flex-start;\n",
    "    margin: 15px 0;\n",
    "}\n",
    "\n",
    ".typing-bubble {\n",
    "    background: white;\n",
    "    padding: 15px 20px;\n",
    "    border-radius: 20px;\n",
    "    margin-right: 30px;\n",
    "    margin-left: 50px;\n",
    "    box-shadow: 0 4px 12px rgba(0,0,0,0.15);\n",
    "}\n",
    "\n",
    ".typing-dots {\n",
    "    display: flex;\n",
    "    gap: 4px;\n",
    "}\n",
    "\n",
    ".dot {\n",
    "    width: 8px;\n",
    "    height: 8px;\n",
    "    background: #999;\n",
    "    border-radius: 50%;\n",
    "    animation: typing 1.4s infinite ease-in-out;\n",
    "}\n",
    "\n",
    ".dot:nth-child(1) { animation-delay: -0.32s; }\n",
    ".dot:nth-child(2) { animation-delay: -0.16s; }\n",
    "\n",
    "@keyframes typing {\n",
    "    0%, 80%, 100% { transform: scale(0.8); opacity: 0.5; }\n",
    "    40% { transform: scale(1); opacity: 1; }\n",
    "}\n",
    "\n",
    ".input-container {\n",
    "    margin-top: 20px;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    ".status-message {\n",
    "    text-align: center;\n",
    "    color: white;\n",
    "    font-style: italic;\n",
    "    margin: 10px 0;\n",
    "    opacity: 0.8;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Helper functions for chatbot display\n",
    "def display_chat_header():\n",
    "    # Display the chatbot header\n",
    "    header_html = f\"\"\"\n",
    "    {chatbot_css}\n",
    "    <div class=\"chat-container\">\n",
    "        <div class=\"chat-header\">\n",
    "             CapitalMind Financial Assistant \n",
    "        </div>\n",
    "        <div class=\"status-message\">\n",
    "            Welcome! Ask me anything about financial markets and stocks.\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(header_html))\n",
    "\n",
    "def display_user_message(message):\n",
    "    # Display user message with chat-bot styling\n",
    "    timestamp = datetime.now().strftime(\"%H:%M\")\n",
    "    user_html = f\"\"\"\n",
    "    <div class=\"chat-container\">\n",
    "        <div class=\"message user-message\">\n",
    "            <div class=\"message-bubble user-bubble\">\n",
    "                {message}\n",
    "                <div class=\"timestamp\">{timestamp}</div>\n",
    "            </div>\n",
    "            <div class=\"message-avatar user-avatar\"></div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(user_html))\n",
    "\n",
    "def display_typing_indicator():\n",
    "    # Display typing indicator (while the Agents are thinking)\n",
    "    typing_html = f\"\"\"\n",
    "    <div class=\"chat-container\">\n",
    "        <div class=\"typing-indicator\">\n",
    "            <div class=\"message-avatar bot-avatar\"></div>\n",
    "            <div class=\"typing-bubble\">\n",
    "                <div class=\"typing-dots\">\n",
    "                    <div class=\"dot\"></div>\n",
    "                    <div class=\"dot\"></div>\n",
    "                    <div class=\"dot\"></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(typing_html))\n",
    "\n",
    "def display_bot_message(message):\n",
    "    # Display bot response with chat-bot styling\n",
    "    timestamp = datetime.now().strftime(\"%H:%M\")\n",
    "    # Format the message with line breaks for better readability\n",
    "    formatted_message = message.replace('\\n', '<br>')\n",
    "    \n",
    "    bot_html = f\"\"\"\n",
    "    <div class=\"chat-container\">\n",
    "        <div class=\"message bot-message\">\n",
    "            <div class=\"message-avatar bot-avatar\"></div>\n",
    "            <div class=\"message-bubble bot-bubble\">\n",
    "                {formatted_message}\n",
    "                <div class=\"timestamp bot-timestamp\">{timestamp}</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(bot_html))\n",
    "\n",
    "def display_status_message(message):\n",
    "    # Display status message\n",
    "    status_html = f\"\"\"\n",
    "    <div class=\"chat-container\">\n",
    "        <div class=\"status-message\">\n",
    "            {message}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(status_html))\n",
    "\n",
    "def clear_typing_indicator():\n",
    "    # Clear the typing indicator in preparation to display the actual reply\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5293fb-b0e0-4933-943a-c573dbe24f52",
   "metadata": {},
   "source": [
    "#### Let's see CapitalMind in action!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1183d45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "<style>\n",
       ".chat-container {\n",
       "    max-width: 800px;\n",
       "    margin: 0 auto;\n",
       "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
       "    border-radius: 20px;\n",
       "    padding: 20px;\n",
       "    box-shadow: 0 10px 30px rgba(0,0,0,0.3);\n",
       "}\n",
       "\n",
       ".chat-header {\n",
       "    text-align: center;\n",
       "    color: white;\n",
       "    font-size: 24px;\n",
       "    font-weight: bold;\n",
       "    margin-bottom: 20px;\n",
       "    text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\n",
       "}\n",
       "\n",
       ".message {\n",
       "    margin: 15px 0;\n",
       "    display: flex;\n",
       "    align-items: flex-start;\n",
       "}\n",
       "\n",
       ".user-message {\n",
       "    justify-content: flex-end;\n",
       "}\n",
       "\n",
       ".bot-message {\n",
       "    justify-content: flex-start;\n",
       "}\n",
       "\n",
       ".message-bubble {\n",
       "    max-width: 70%;\n",
       "    padding: 15px 20px;\n",
       "    border-radius: 20px;\n",
       "    box-shadow: 0 4px 12px rgba(0,0,0,0.15);\n",
       "    position: relative;\n",
       "    line-height: 1.4;\n",
       "}\n",
       "\n",
       ".user-bubble {\n",
       "    background: linear-gradient(135deg, #36d1dc 0%, #5b86e5 100%);\n",
       "    color: white;\n",
       "    margin-left: 30px;\n",
       "}\n",
       "\n",
       ".bot-bubble {\n",
       "    background: white;\n",
       "    color: #333;\n",
       "    margin-right: 30px;\n",
       "    border: 1px solid #e0e0e0;\n",
       "}\n",
       "\n",
       ".message-avatar {\n",
       "    width: 40px;\n",
       "    height: 40px;\n",
       "    border-radius: 50%;\n",
       "    margin: 0 10px;\n",
       "    display: flex;\n",
       "    align-items: center;\n",
       "    justify-content: center;\n",
       "    font-weight: bold;\n",
       "    color: white;\n",
       "    font-size: 16px;\n",
       "}\n",
       "\n",
       ".user-avatar {\n",
       "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
       "}\n",
       "\n",
       ".bot-avatar {\n",
       "    background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
       "}\n",
       "\n",
       ".timestamp {\n",
       "    font-size: 11px;\n",
       "    color: rgba(255,255,255,0.7);\n",
       "    margin-top: 5px;\n",
       "    text-align: right;\n",
       "}\n",
       "\n",
       ".bot-timestamp {\n",
       "    color: #999;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".typing-indicator {\n",
       "    display: flex;\n",
       "    align-items: center;\n",
       "    justify-content: flex-start;\n",
       "    margin: 15px 0;\n",
       "}\n",
       "\n",
       ".typing-bubble {\n",
       "    background: white;\n",
       "    padding: 15px 20px;\n",
       "    border-radius: 20px;\n",
       "    margin-right: 30px;\n",
       "    margin-left: 50px;\n",
       "    box-shadow: 0 4px 12px rgba(0,0,0,0.15);\n",
       "}\n",
       "\n",
       ".typing-dots {\n",
       "    display: flex;\n",
       "    gap: 4px;\n",
       "}\n",
       "\n",
       ".dot {\n",
       "    width: 8px;\n",
       "    height: 8px;\n",
       "    background: #999;\n",
       "    border-radius: 50%;\n",
       "    animation: typing 1.4s infinite ease-in-out;\n",
       "}\n",
       "\n",
       ".dot:nth-child(1) { animation-delay: -0.32s; }\n",
       ".dot:nth-child(2) { animation-delay: -0.16s; }\n",
       "\n",
       "@keyframes typing {\n",
       "    0%, 80%, 100% { transform: scale(0.8); opacity: 0.5; }\n",
       "    40% { transform: scale(1); opacity: 1; }\n",
       "}\n",
       "\n",
       ".input-container {\n",
       "    margin-top: 20px;\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       ".status-message {\n",
       "    text-align: center;\n",
       "    color: white;\n",
       "    font-style: italic;\n",
       "    margin: 10px 0;\n",
       "    opacity: 0.8;\n",
       "}\n",
       "</style>\n",
       "\n",
       "    <div class=\"chat-container\">\n",
       "        <div class=\"chat-header\">\n",
       "             CapitalMind Financial Assistant \n",
       "        </div>\n",
       "        <div class=\"status-message\">\n",
       "            Welcome! Ask me anything about financial markets and stocks.\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"chat-container\">\n",
       "        <div class=\"message user-message\">\n",
       "            <div class=\"message-bubble user-bubble\">\n",
       "                Based on the latest news, is it a good time to buy Apple stock?\n",
       "                <div class=\"timestamp\">17:23</div>\n",
       "            </div>\n",
       "            <div class=\"message-avatar user-avatar\"></div>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"chat-container\">\n",
       "        <div class=\"message bot-message\">\n",
       "            <div class=\"message-avatar bot-avatar\"></div>\n",
       "            <div class=\"message-bubble bot-bubble\">\n",
       "                Excellent question! Based on the comprehensive financial information available, there are several compelling indicators suggesting a robust outlook for Apple's stock. Market sentiment is predominantly positive, reinforced by a strong consensus among analysts, with a significant majority recommending \"Buy\" or \"Strong Buy.\" This confidence appears well-founded, as Apple has demonstrated strong stock performance, gaining over 19% in the last quarter, with its current price comfortably above key moving averages, signaling a bullish trend.<br><br>This positive momentum is underpinned by strategic corporate developments. Apple is actively diversifying its revenue streams through significant investments, such as securing a five-year deal for U.S. streaming rights to Formula One races for Apple TV, indicating a strong push into high-margin services and live sports content. Additionally, the company is pursuing balanced global growth through strategic investments in both the U.S. and China, aiming for supply chain resilience. Financially, Apple has consistently met or exceeded earnings expectations in recent quarters, including a notable positive surprise in Q3 2025, further contributing to a stable and positive market perception.<br><br>Given these strong positive signalsincluding robust analyst confidence, impressive recent stock performance, strategic growth initiatives, and consistent financial executionthe information points to a favorable environment for Apple stock. However, as with any investment decision, it is always prudent to consider your personal financial goals and risk tolerance. While the current outlook appears strong based on the provided insights, potential investors should always conduct their own thorough due diligence and consider market dynamics that extend beyond these specific data points.\n",
       "                <div class=\"timestamp bot-timestamp\">17:23</div>\n",
       "            </div>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Your question:  exit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"chat-container\">\n",
       "        <div class=\"message user-message\">\n",
       "            <div class=\"message-bubble user-bubble\">\n",
       "                exit\n",
       "                <div class=\"timestamp\">17:25</div>\n",
       "            </div>\n",
       "            <div class=\"message-avatar user-avatar\"></div>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"chat-container\">\n",
       "        <div class=\"status-message\">\n",
       "             Exiting the financial market assistant. Goodbye!\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying the header and launching the chat bot.\n",
    "display_chat_header()\n",
    "\n",
    "## Create a conversation loop with user input and orchestrator response\n",
    "conversation_history = []\n",
    "#Defining an infinite loop that will allow the user to have a conversation (until the user types the command \"exit\")\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\" Your question: \")\n",
    "        \n",
    "        # Display user message with beautiful styling\n",
    "        display_user_message(user_input)\n",
    "        conversation_history.append(f\"User: {user_input}\")\n",
    "\n",
    "        # Evaluating if the user entered the exit command first, in which case the conversation is over.\n",
    "        if user_input.lower() == 'exit':\n",
    "            display_status_message(\" Exiting the financial market assistant. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Show typing indicator while processing\n",
    "        display_typing_indicator()\n",
    "        time.sleep(1)  # Brief pause for realistic effect\n",
    "        \n",
    "        # Get response from orchestrator\n",
    "        response = MyOrchestrator.reAct(user_input)  # Pass just the user input, not the full history\n",
    "        conversation_history.append(f\"Orchestrator: {response}\") # Capturing the Agents' response in the conversatio history\n",
    "        \n",
    "        # Clear typing indicator and display bot response\n",
    "        clear_output(wait=True) #Clearing the output for next conversation\n",
    "        display_chat_header() #Displaying the header again (restarting the conversation)\n",
    "        \n",
    "        # Redisplay recent conversation\n",
    "        recent_history = conversation_history[-6:]  # Show last 3 exchanges\n",
    "        for i in range(0, len(recent_history), 2):\n",
    "            if i < len(recent_history):\n",
    "                # Display user message\n",
    "                user_msg = recent_history[i].replace(\"User: \", \"\")\n",
    "                display_user_message(user_msg)\n",
    "            if i + 1 < len(recent_history):\n",
    "                # Display bot message\n",
    "                bot_msg = recent_history[i + 1].replace(\"Orchestrator: \", \"\")\n",
    "                display_bot_message(bot_msg)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        display_status_message(\" Chat interrupted. Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        display_bot_message(f\" Sorry, I encountered an error: {str(e)}\")\n",
    "        display_status_message(\"Please try asking your question again.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a113ed-09dc-4d20-ba76-baa879315c15",
   "metadata": {},
   "source": [
    "## CONCLUSION.\n",
    "#### The usage of Agentic AI systems is significantly more powerful than other generative AI techniques because it lets us specialize different tasks, separating the planning from the research and the response writing, providing flexibility to the complexity of the user's dynamic inquiry.\n",
    "#### From the programming perspective, it also provides modularity, explainability, and transparency. This allows for flexibility as well in terms of expansion or upgrade of the skills of the solution, as well as making the maintenance of the tools more easily.\n",
    "#### This financial exercise was great to understand the full process of designing and implementing an Agentic AI system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

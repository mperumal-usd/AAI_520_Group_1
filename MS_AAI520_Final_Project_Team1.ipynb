{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7846f45-e90c-44c2-8ad1-e1f8db4e3325",
   "metadata": {},
   "source": [
    "# UNIVERSITY OF SAN DIEGO - MS AAI\n",
    "## Natural Language Processing and Generative AI\n",
    "### Final Project - Team 1: Multi-Agent Financial Analysis System.\n",
    "#### By Manikandan Perumal & Israel Romero Olvera\n",
    "#### _________________________________________________\n",
    "#### The purpose of this final project is to build a real-world financial analysis system powered by agentic AI, with the abilities of reasoning, planning, and acting based on the user's prompt. It will coordinate multiple specialized LLM agents to handle complex financial tasks end-to-end.\n",
    "#### Our Agentic AI system was developed in a folder structure that can be found in our GitHub site: https://github.com/isralennon/AAI_520_Group_1/tree/main\n",
    "#### For delivery purposes we've condensed all the code into this document, structured the following way:\n",
    "#### 1. Tools - this section contains the code in file /modules/tools.py which will perform basic RAG connections.\n",
    "#### 2. Parser - this section contains the code in file /modules/parser.py, which provides basic functionality to parse data in JSON format.\n",
    "#### 3. Memory - this section contains the code in file /modules/memory.py that handles the storage of ongoing knowledge, to provide a robust and efficient functionality.\n",
    "#### 4. Agents - this section contains the code in file /modules/subagents.py, designed to host the definitions of the main Agent class as well as our specialized subagents - the team of agents available to the main orchestrator. We developed the following team of agents:\n",
    "#### - Orchestrator - the \"Manager\" of the Agents\n",
    "#### - News Researcher - the specialist of finding financial news, using FinnHub.\n",
    "#### - Market Researcher - the specialist of finding financial hard data like market trends, stock prices, etc.\n",
    "#### - Writer - the specialist of taking all the information and preparing a polished answer for the user\n",
    "#### 5. Main Orchestrator Agent - this section contains the code in file /modules/agent.py and has the definition for the orchestrator agent, which develops the strategy and coordinates all subagents.\n",
    "#### 6. Demo - this section contains the code in our main notebook, /main.ipynb - our implementation file where we execute all the above with demonstration purposes.\n",
    "#### _________________________________________________\n",
    "### 1. TOOLS.\n",
    "#### One of the four agent functions we'll implement is the usage of tools, which will be defined in this first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5847078-4728-443e-9d7f-3a350066678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isral\\.conda\\envs\\Transformers_3_10\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "#import modules.tools as tools\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import finnhub\n",
    "from typing import Callable\n",
    "from datetime import datetime, timedelta\n",
    "from google import genai\n",
    "import openai\n",
    "# For privacy reasons, we'll store our token keys on a .env file, which we'll load here:\n",
    "dotenv.load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "\n",
    "\n",
    "# First, we'll define a generic Tool class, which will serve as a structure for all of our tools\n",
    "class Tool:\n",
    "    def __init__(self, name, function, description, api=None): # This is the initialization method of the class\n",
    "        self.name = name # Placeholder for the name of the tool\n",
    "        self.function = function # Placeholder for the code of the tool's function\n",
    "        self.description = description # Placeholder for the description of the tool - very important since the agents will use this description to know what the tool does\n",
    "        self.api = api  # Placeholder for API details when needed\n",
    "        \n",
    "    def to_dict(self): # The structure of each class will always be a standard dictionary object that can be easily interpreted by the Agents\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"api\": self.api\n",
    "        }\n",
    "    \n",
    "    def invoke(self, **kwargs): # This is the placeholder of the function for the tool, which will receive a variable number of parameters\n",
    "        print(f\"Invoking {self.name} with arguments {kwargs}\")\n",
    "        return self.function(**kwargs) # Returning the results of the function\n",
    "\n",
    "# Next, we'll declare each individual tool as a class, inheriting from the generic class Tool above\n",
    "class YahooFinance(Tool): # The first tool is YahooFinance, which will pull stock quotes for a given financial symbol, like AAPL for Apple\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Yahoo Finance Stock Quote\", # Name of the tool\n",
    "            function=self.get_stock_quote_yahoo, # Pointing to the YahooFinance function below as this class's own function\n",
    "            description=\"Get the latest stock quote for a given symbol from Yahoo Finance.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "        )\n",
    "    def get_stock_quote_yahoo(self, symbol: str, step: str='') -> dict: # This is the function that pulls the stock using YahooFinance API\n",
    "        # Here we'll perform the call to YahooFinance to get the data from the specified symbol.\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        # Then, we'll use the 'fast_info' method, which pulls basic financial information, including the price.\n",
    "        try:\n",
    "            info = ticker.fast_info # Pulling the information and parsing it to return it\n",
    "            return {\n",
    "                \"symbol\": symbol,\n",
    "                \"last_price\": info[\"lastPrice\"],\n",
    "                \"day_high\": info[\"dayHigh\"],\n",
    "                \"day_low\": info[\"dayLow\"],\n",
    "                \"previous_close\": info[\"previousClose\"]\n",
    "            }\n",
    "        except Exception as e: # Should there be any errors, we will print the error message instead and return an empty dictionary\n",
    "            print(f\"Yahoo Finance API error: {e}\")\n",
    "            return {}\n",
    "#Now, we'll continue with the class that calls Financial Modeling Prep API\n",
    "class FMP(Tool):\n",
    "    def __init__(self,name:str,function:Callable=None,description:str=None,api:str=None,endPoint:str=None):\n",
    "        super().__init__(name=name,function=self.execute if function==None else function,description=description,api=api)\n",
    "        self.endpoint = endPoint if endPoint!=None else  os.getenv(\"FMP_Endpoint\") # It reads the endpoint from our .env file\n",
    "        self.apikey = os.getenv(\"FMP_API_KEY\") # It also reads the API key from our .env file\n",
    "    def execute(self, symbol: str) -> dict: # This is the function that pulls the stock data using FMP API\n",
    "        params = { #These are the parameters for the API call in a dictionary format\n",
    "            \"symbol\": symbol,\n",
    "            \"apikey\": self.apikey,\n",
    "            \"exchange\": \"NASDAQ\"\n",
    "        }\n",
    "        try: #Then we'll try to make the call to the API and return its formatted response as a JSON text\n",
    "            # print(f'Calling FMP API at endpoint: {self.endpoint} with params: {params}')\n",
    "            response=requests.get(self.endpoint, params=params)\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'FMP API error: {e}')\n",
    "            return {}\n",
    "        \n",
    "class StockQuote(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Stack Quote\", # Name of the tool\n",
    "            description=\"Get the latest stock quote for a given symbol from Stack Quote.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/quote' # It reads the endpoint from our .env file\n",
    "        )\n",
    "\n",
    "        \n",
    "class StockPriceChange(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Stock Price Change\", # Name of the tool\n",
    "            description=\"Get the stock price change for a given symbol over the past.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\", \"days\": 7}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/stock-price-change' # It reads the endpoint from our .env file\n",
    "        )\n",
    "        \n",
    "class IncomeStatement(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Income Statement\", # Name of the tool\n",
    "            description=\"Get the income statement for a given symbol from Financial Modeling Prep (FMP).\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/income-statement' # It reads the endpoint from our .env file\n",
    "        )\n",
    "  \n",
    "class FinancialScore(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Financial Score\", # Name of the tool\n",
    "            description=\"Get the financial score for a given symbol from Financial Modeling Prep (FMP).\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\" ,# Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/financial-scores' # It reads the endpoint from our .env file\n",
    "        )\n",
    "   \n",
    "        \n",
    "#We'll be using FinnHub as our News provider next\n",
    "class FinancialNews(Tool): \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub News\", # Name of the tool\n",
    "            function=self.get_stock_quote_finnhub, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the latest financial news for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when it uses this class\n",
    "        )\n",
    "    def get_stock_quote_finnhub(self, symbol: str, step: str='') -> dict: # This is the function that pulls the news data using FinnHub\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        # Next, we setup the client to perform calls:\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "\n",
    "        # Setting a time frame for the news, ending today and starting a week ago\n",
    "        end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "        start_date = (datetime.today() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Now, we call the API, returning the news in the already pre-formatted dictionary structure.\n",
    "        try:\n",
    "            news= finn_client.company_news(symbol, _from=start_date, to=end_date)\n",
    "            if len(news)==0:\n",
    "                return {\"message\": f\"No news found for symbol {symbol} from {start_date} to {end_date}.\"}\n",
    "            top_news = sorted(news, key=lambda x: x['datetime'], reverse=True)[:5]\n",
    "            top_news_formatted = []\n",
    "            for item in top_news:\n",
    "                top_news_formatted.append({\n",
    "                    \"headline\": item.get(\"headline\"),\n",
    "                    \"summary\": item.get(\"summary\"),\n",
    "                    \"datetime\": datetime.fromtimestamp(item.get(\"datetime\")).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "        \n",
    "            return {\n",
    "                \"symbol\": symbol,\n",
    "                \"news\": top_news_formatted  \n",
    "            }\n",
    "    \n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}\n",
    "\n",
    "class RecommendationTrends(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub Recommendation Trends\", # Name of the tool\n",
    "            function=self.get_recommendation_trends, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the recommendation trends for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when this class\n",
    "        )\n",
    "    def get_recommendation_trends(self, symbol: str) -> dict:\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "        try:\n",
    "            return finn_client.recommendation_trends(symbol)\n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}\n",
    "        \n",
    "class EarningSurprise(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub Earning Surprise\", # Name of the tool\n",
    "            function=self.get_earning_surprise, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the earning surprise for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when this class\n",
    "        )\n",
    "    def get_earning_surprise(self, symbol: str) -> dict:\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "        try:\n",
    "            return finn_client.company_earnings(symbol,limit=5)\n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd7b2a-b7e4-45df-8551-153c0d0da84b",
   "metadata": {},
   "source": [
    "### 2. PARSER\n",
    "#### One of the workflow patterns our agents will do is routing, meaning our main agent will coordinate with subagents. To accomplish this communication, we need a \"common language\", which in this case will be JSON. This section defines the functions to implement the JSON parsing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5fdb39-7ca7-4dca-8fb9-6173fa06dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll define first a Parser abstract class\n",
    "class Parser:\n",
    "    def parse(self, response): # This is the placeholder of the default method for this class\n",
    "        # Here's the returned value, which will be a dictionary with an Action value, and a list of dynamic parameters.\n",
    "        return {\"action\": \"FinalAnswer\", \"parameters\": {}}\n",
    "# Next, we'll define an XML parser, which inherits from our abstract class Parser.    \n",
    "class XmlParser(Parser):\n",
    "    def parse(self, response):\n",
    "        # A parser that extracts XML tags from the response.\n",
    "        # For example, it looks for <InvokeTool>{\"symbol\": \"AAPL\", \"step\": \"financials\"}</InvokeTool>\n",
    "        # or <FinalAnswer>answer</FinalAnswer>.\n",
    "        # Returns : a dict with action and parameters. Example:\n",
    "        # {\n",
    "        #    \"action\": \"InvokeTool\",\n",
    "        #    \"parameters\": {\n",
    "        #        \"symbol\": \"AAPL\",\n",
    "        #        \"step\": \"financials\"\n",
    "        #    }\n",
    "        #}\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>' # Defining the regular expression for XML structure\n",
    "        matches = re.findall(pattern, response) # Identifying all matches of XML\n",
    "        if matches: # When there are XML matches, we'll separate them and parse their contents\n",
    "            action, content = matches[0]\n",
    "            content = content.strip()\n",
    "            contentJson = {}\n",
    "            try:\n",
    "                import json\n",
    "                contentJson = json.loads(content) # Once parsed, we'll reformat them to JSON\n",
    "            except:\n",
    "                contentJson = {\"content\": content} # If the content is not valid, we'll return the error message with the invalid content text\n",
    "                return {\"action\": action, \"parameters\": contentJson, \"error\": \"Content is not valid JSON\"}\n",
    "            return {\"action\": action, \"parameters\": contentJson} # If it was valid, we return the parsed content in JSON format\n",
    "        return {\"action\": \"FinalAnswer\", \"parameters\": {}} #If there wasn't any XML to begin with, we just return an empty list of parameters\n",
    "    # Next, we have a specialized parsing for our agent's functionality that will interpret the actions in XML tags and encode them as a list of dictionaries\n",
    "    def  parse_all(self, response):\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>' # Defining the regular expression for XML structure\n",
    "        matches = re.findall(pattern, response) # Identifying all matches of XML\n",
    "        results = [] # Preparing an empty array for the results\n",
    "        for action, content in matches: # For each detected action (if any),\n",
    "            content = content.strip()   # we'll parse its contents\n",
    "            contentJson = {}\n",
    "            try: # Then, we'll try to convert it to JSON format\n",
    "                import json\n",
    "                contentJson = json.loads(content)\n",
    "            except: # Should any errors occur, we'll return the error message as part of the response\n",
    "                contentJson = {\"content\": content}\n",
    "                results.append({\"action\": action, \"parameters\": contentJson, \"error\": \"Content is not valid JSON\"})\n",
    "                continue\n",
    "            results.append({\"action\": action, \"parameters\": contentJson}) # If everything's fine, we'll return the parsed JSON content\n",
    "        if not results:\n",
    "            results.append({\"action\": \"FinalAnswer\", \"parameters\": {}}) # If there were no actions, we'll return an empty dictionary\n",
    "        return results  \n",
    "    \n",
    "    def parseTags(self, response):\n",
    "        '''Agent response parser to extract all TAGS.\n",
    "            Returns a dictionary with tag names as keys and tag values as values.\n",
    "        '''\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>'\n",
    "        matches = re.findall(pattern, response)\n",
    "        result = {}\n",
    "        for tag, value in matches:\n",
    "                result[tag.lower()] = value.strip() \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0e958-92da-4165-acd4-d541e967c259",
   "metadata": {},
   "source": [
    "### 3. MEMORY.\n",
    "#### Another feature of our agent is learning, which means the agent must remember information as it gets prompted to refine their answers and keep getting more knowledgeable as it gets used. The functions that perform such learning are defined in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5315f5-f806-4c34-87f7-f4187f6baa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "# We're creating a class called MemorySystem with all the learning functionality\n",
    "class MemorySystem:\n",
    "    # This class stores insights and lessons from previous analyses to improve future runs.\n",
    "    def __init__(self, memory_file='agent_memory.pkl'): # It will store the learned data into the specified file, or the default file name.\n",
    "        self.memory_file = memory_file\n",
    "        self.stock_insights = {}\n",
    "        self.news_insights = {}\n",
    "        self.load_memory()\n",
    "    \n",
    "    def load_memory(self): # Should there be a previous file in existence, it can load it using this function\n",
    "        try:\n",
    "            if os.path.exists(self.memory_file): # It will look for the file name specified in the instance of this class\n",
    "                with open(self.memory_file, 'rb') as f: # If it exists, it will attempt to open it\n",
    "                    memory_data = pickle.load(f) # Then, it will load the data into memory\n",
    "                    self.stock_insights = memory_data.get('stock_insights', {}) # separating stock insights,\n",
    "                    self.news_insights = memory_data.get('news_insights', {}) # market news insights,\n",
    "            else: # Should there be no prior file, it will start fresh\n",
    "                print(\"No memory file found. Starting with empty memory.\")\n",
    "        except Exception as e: # Should there be an error while loading the file, it will start fresh as well\n",
    "            print(f\"Error loading memory: {e}\")\n",
    "            print(\"Starting with empty memory.\")\n",
    "    \n",
    "    def save_memory(self): # This method will save the memory in the file in a structured manner\n",
    "        try:\n",
    "            memory_data = {\n",
    "                'stock_insights': self.stock_insights, # It will save all stock insights currently provided,\n",
    "                'news_insights': self.news_insights # followed by news insights\n",
    "            }\n",
    "            with open(self.memory_file, 'wb') as f: # It will first open the file name specified in the instance of this class\n",
    "                pickle.dump(memory_data, f) # and then write in it the contents of the memory_data dictionary\n",
    "            print(\"Memory saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving memory: {e}\") # Should there be any errors saving, it will print out the error\n",
    "    \n",
    "    def add_stock_insight(self, symbol, insight, timestamp=None): # With this method, we'll add knowledge classified as stock insights\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().isoformat() # If no timestamp is specified, we'll initialize the current time stamp\n",
    "        \n",
    "        if symbol not in self.stock_insights: # If the current symbol (financial company) is not in previous insights, we'll add it\n",
    "            self.stock_insights[symbol] = []\n",
    "        \n",
    "        self.stock_insights[symbol].append({ # Finally, we encode the insight with its timestamp in the stock_insights dictionary of this class\n",
    "            'insight': insight,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        self.save_memory() # And we save the memory right away\n",
    "    \n",
    "    def add_market_news(self,symbol, news_item, timestamp=None): # This method adds market news insights for a given symbol\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().isoformat() # If no timestamp is specified, we'll initialize the current time stamp\n",
    "\n",
    "        if symbol not in self.news_insights: # If the current symbol (financial company) is not in previous insights, we'll add it\n",
    "            self.news_insights[symbol] = []\n",
    "\n",
    "        self.news_insights[symbol].append({ # Finally, we encode the news item with its timestamp in the news_insights dictionary of this class\n",
    "            'news_item': news_item,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        self.save_memory() # And we save the memory right away\n",
    "\n",
    "    def get_stock_insights(self, symbol): # This method retrieves all stock insights for a given symbol\n",
    "        results=self.stock_insights.get(symbol, [])\n",
    "        if not results:\n",
    "            print(f\"No insights found for symbol {symbol}.\")\n",
    "            return []\n",
    "        if results:\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                # if the timestamp is older than 7 days, we can choose to ignore it\n",
    "                timestamp = datetime.fromisoformat(result['timestamp'])\n",
    "                if (datetime.now() - timestamp).days > 7:\n",
    "                    continue\n",
    "                filtered_results.append(result)\n",
    "        return filtered_results\n",
    "\n",
    "    def get_news_insights(self, symbol): # This method retrieves all market news insights for a given symbol\n",
    "        results=self.news_insights.get(symbol, [])\n",
    "        if not results:\n",
    "            print(f\"No news insights found for symbol {symbol}.\")\n",
    "            return []\n",
    "        if results:\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                # if the timestamp is older than 2 days, we can choose to ignore it\n",
    "                timestamp = datetime.fromisoformat(result['timestamp'])\n",
    "                if (datetime.now() - timestamp).days > 2:\n",
    "                    continue\n",
    "                filtered_results.append(result)\n",
    "        return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf287589-30f8-4c56-a61c-175eeaf9af11",
   "metadata": {},
   "source": [
    "### 4. AGENTS\n",
    "#### For our routing workflow, along with communication also comes specialization and tool usage: a team of agents that will collaborate, coordinated by the main orchestrator agent. That's what we'll define in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13a923fc-91cd-40c0-aeb6-55eb20d0bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we'll initialize the Google GenAI and OpenAI\n",
    "from google import genai\n",
    "import openai\n",
    "#Make sure to load the environmental variables\n",
    "dotenv.load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "# Downloading necessary libraries and functionality - uncomment when needed.\n",
    "#nltk.download('vader_lexicon')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "class Agent: # This will be our base class for all our agents\n",
    "    def __init__(self, name, role, system_prompt, model, generate_response, agents=None, tools=None, memory_system=None, parser=None): # This is the initialization method of the Agent class\n",
    "        self.name = name # Placeholder for the name of the tool\n",
    "        self.model = model # Placeholder for the LLM model\n",
    "        self.role = role # Placeholder for the role of this agent\n",
    "        self.system_prompt = system_prompt # Placeholder for the system prompt that defines this agent\n",
    "        self.memory_system = memory_system # Placeholder for the memory object for this agent - it could be None, so the agent would start without knowledge\n",
    "        self.parser = parser  # Placeholder for API details when needed\n",
    "        self.generate_response = generate_response # Placeholder for the generate response method\n",
    "        self.agents = agents \n",
    "        self.tools = tools # Placeholder for the tools passed on to this agent, which should be a list\n",
    "        self.conversation_history = [] # Initializing a blank conversation history\n",
    "        self.max_history_length = 10 # Initializing a default max number of history length\n",
    "\n",
    "        self.prompt_template = (\n",
    "            \"You are {agent_name}, an AI agent. Use the following tools as needed:\\n\"\n",
    "            \"{tools}\\n\"\n",
    "            \"Conversation history:\\n\"\n",
    "            \"{history}\\n\"\n",
    "            \"Current input: {input}\\n\"\n",
    "            \"Respond appropriately.\"\n",
    "        )\n",
    "        self.initialize_client()\n",
    "    #We want our Agent class to support multiple LLMs, so this function will help initialize its internal client dynamically.\n",
    "    def initialize_client(self):\n",
    "        #For GPT models\n",
    "        if \"gpt\" in self.model.lower(): \n",
    "            self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        #For Gemini models\n",
    "        elif \"gemini\" in self.model.lower():\n",
    "            self.client = genai.Client()\n",
    "    def to_dict(self): # The structure of each class will always be a standard dictionary object that can be easily interpreted by the Agents\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"api\": self.api\n",
    "        }\n",
    "    def register_tool(self, tool):\n",
    "        self.tools.append(tool)\n",
    "    def remember(self, message):\n",
    "        self.conversation_history.append(message)\n",
    "        if len(self.conversation_history) > self.max_history_length:\n",
    "            self.conversation_history.pop(0)\n",
    "    def call_llm(self, input_prompt):\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": input_prompt}\n",
    "                    ],\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                prompt = self.system_prompt\n",
    "                prompt += \"\\n\" + input_prompt\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            #print(f\"{self.name} using model '{self.model}': {result[:60]}...\")\n",
    "            #print(result)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "    def generate_response(self, **kwargs): # This is the placeholder of the generative function for the agent, which will receive a variable number of parameters\n",
    "        print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        return self.generate_response(**kwargs) # Returning the results of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed17b1-9690-4054-8131-025b22fe39fb",
   "metadata": {},
   "source": [
    "#### Next, let's define some Sub-agents that will inherit from the class above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ac90380-12c0-4734-8642-c0c0445c230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriterAgent(Agent):\n",
    "    # This agent takes the results of other agents (like news or market research) and creates a professional report that will be returned to the Orchestrator for the Final Response to the user.\n",
    "    def __init__ (self, model=\"gpt-3.5-turbo\", agents=None, memory_system=None):\n",
    "        super().__init__(\n",
    "            name=\"Writer\", #Name of the Writer class\n",
    "            role=\"Writer Agent specialized in polished Financial Content\", #Role of this class\n",
    "            system_prompt=(\n",
    "                \"You are Writer, an AI agent part of an agents team. Your role is a professional \"\n",
    "                \"financial report writer, capable of taking financial news \"\n",
    "                \"or financial information provided by the Orchestrator and \"\n",
    "                \"preparing a 2â€“3 paragraph report that provides a clear final \"\n",
    "                \"answer to the user.\"\n",
    "                \"Here are some guidelines for you:\"\n",
    "                \"Start your answers giving a positive message like 'Great question', 'Excellent question', or similar.\"\n",
    "                \"Focus on answering the user's question.\"\n",
    "                \"When recommendations are requested, only provide guidance and highlight pros and cons.\"\n",
    "                \"The news or financial information you're receiving came from other agents in the team, so never refer to it as 'the data provided'.\"\n",
    "            ),            \n",
    "            model = model,\n",
    "            generate_response = self.generate_response,\n",
    "            memory_system = memory_system\n",
    "        )\n",
    "    def generate_response(self, input_prompt):\n",
    "        result = self.call_llm(input_prompt)\n",
    "        return result\n",
    "    def processUserInput(self, input_prompt: str) -> str:\n",
    "        prompt=input_prompt\n",
    "        response=self.generate_response(input_prompt=prompt)\n",
    "        return response    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9131fe8-cb38-4132-a7a7-76aae303751b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Excellent question! Determining whether it's a good time to buy Apple stock involves weighing various factors, including recent developments and market movements.\\n\\nOn the positive side, Apple's deep engagement in artificial intelligence, including plans for smart glasses and AI integration across its product ecosystem, presents significant growth opportunities and demonstrates a commitment to innovation. The strong demand for the iPhone 17 also indicates continued consumer interest in its core product lines. Coupled with a slight increase in its stock price from $254.04 on October 17, 2025, to $255.74 on October 18, 2025, there are indications of ongoing positive momentum.\\n\\nHowever, potential investors should also consider the challenges. Some analysts express caution regarding future iPhone models' expectations, suggesting that long-term growth might face headwinds. Furthermore, Apple is currently under legal scrutiny concerning chip royalties and data collection practices, which could introduce regulatory risks and potential financial impacts. When making investment decisions, it's crucial to balance these growth catalysts with the potential risks and to conduct thorough personal research.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use \"gpt-3.5-turbo\" or \"gemini-2.5-flash\", #Uncomment if using Google GenAI\n",
    "MyWriter = WriterAgent(model=\"gemini-2.5-flash\")\n",
    "\n",
    "sample_prompt = (\"Orchestrator: The user wants to know if, based on the latest news and stock prices it is a good time to buy Apple stock. \"\n",
    "                    \"Here are the latest news: Apple is deeply involved in AI, planning smart glasses and integrating AI into its products. The iPhone 17 is seeing strong demand, though some analysts are cautious about future models' expectations. Apple faces legal scrutiny regarding chip royalties and data collection.\"\n",
    "                    \"Here are stock prices for the past 2 days: The stock price on 10/17/2025 was **$254.04**. The stock price on 10/18/2025 is **$255.74**\"\n",
    "                )\n",
    "\n",
    "MyWriter.processUserInput(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94b97db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketResearchAgent(Agent):\n",
    "    def __init__(self,model=\"gemini-2.5-flash\"):\n",
    "        name=\"Market Research Agent\"\n",
    "        model=model\n",
    "        role=\"Market Research Agent specialized in financial data analysis and market trends\"\n",
    "        system_prompt=f\"\"\"You are a Market Research Agent specialized in financial data analysis and market trends.\n",
    "         Your role is to assist users by providing accurate and up-to-date financial information, stock quotes, market trends, and insights based on the latest data available from various financial APIs and tools.\n",
    "\n",
    "         Based on the data retrieved from the tools at your disposal, provide comprehensive answers to user queries related to stock performance, market analysis, and financial news.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.memory_system=MemorySystem()\n",
    "        super().__init__(name=name,system_prompt=system_prompt,model=model,generate_response=self.generate_response,role=role,agents=None,tools=None,memory_system=self.memory_system,parser=None) \n",
    "    \n",
    "    def generate_response(self, **kwargs): # This is the placeholder of the generative function for the agent, which will receive a variable number of parameters\n",
    "        # print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        input_prompt=kwargs.get(\"prompt\",[])\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": input_prompt}\n",
    "                    ],\n",
    "                    #messages=input_prompt,\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "\n",
    "    def getMarketSummary(self,symbol:str  ) -> str:\n",
    "        prompt=f\"\"\"Provide a comprehensive market summary for the stock symbol: {symbol}. \n",
    "                Include recent performance, key financial metrics, and any notable news or trends affecting the stock.\n",
    "                Use data from Yahoo Finance, Financial Modeling Prep, and FinnHub to inform your summary.\n",
    "                Format the response in a clear and concise manner suitable for a financial report.\"\"\"\n",
    "        insights = self.memory_system.get_stock_insights(symbol)\n",
    "        if insights:\n",
    "            print(f\"Using cached insight for symbol {symbol}.\")\n",
    "            return insights[-1]['insight']\n",
    "        else:\n",
    "            tools_list=[FinancialScore(),IncomeStatement(),StockQuote(),StockPriceChange()]\n",
    "            for tool in tools_list:\n",
    "                tool_response=tool.invoke(symbol=symbol)\n",
    "                prompt+=f\"\\nData from {tool.name}: {tool_response}\"\n",
    "            response=self.generate_response(prompt=prompt)\n",
    "            self.memory_system.add_stock_insight(symbol, response,timestamp=datetime.now().isoformat())\n",
    "        return response  \n",
    "    \n",
    "    def  processUserInput(self, user_input: str) -> str:\n",
    "        tags=self.getEntities(user_input=user_input)\n",
    "        if \"symbol\" in tags:\n",
    "            marketSummary=self.getMarketSummary(symbol=tags.get(\"symbol\"))\n",
    "        prompt=f\"\"\"Based on the {marketSummary} Analyze the following user input\n",
    "                and provide a short answer for the user query.\n",
    "                Rules:\n",
    "                - If the user input is related to stock performance, provide insights based on the market summary.\n",
    "                - If the user input is unrelated to financial markets, respond with \"I'm sorry, I can only assist with financial market-related queries.\"\n",
    "                - Keep the response concise and relevant to the user's query.\n",
    "                - Use a professional and informative tone suitable for financial discussions.\n",
    "                - Limit the response to 150 words.\n",
    "\n",
    "                User Input: \"{user_input}\"\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\",\n",
    "        response=self.generate_response(prompt=prompt)\n",
    "        return response\n",
    "\n",
    "    def getEntities(self, user_input: str) -> str:\n",
    "        prompt=f\"\"\"Determine entities the following user input related to financial markets and stock analysis:\n",
    "                if the input contains Apple Inc, return SYMBOL as AAPL\n",
    "                if the input contains Microsoft Corporation, return SYMBOL as MSFT\n",
    "                User Input: \"{user_input}\n",
    "                Extracted Entities:\n",
    "                    <SYMBOL>...</SYMBOL>\n",
    "                    <EXCHANGE>...</EXCHANGE><INDUSTRY>...</INDUSTRY>  \"\"\"\n",
    "        response=self.generate_response(prompt=prompt)\n",
    "        print(f'Response: {response}')\n",
    "        parser=XmlParser()\n",
    "        parsed_response=parser.parseTags(response)\n",
    "        return parsed_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4862007d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Extracted Entities:\n",
      "<SYMBOL>TSLA</SYMBOL>\n",
      "<EXCHANGE>NASDAQ</EXCHANGE><INDUSTRY>Automotive</INDUSTRY>\n",
      "Using cached insight for symbol TSLA.\n",
      "The current market price for TSLA is $439.31. Based on the comprehensive market summary, Tesla's P/E ratio is notably high at 215.35x, reflecting a premium valuation driven by investor expectations for high future growth.\n",
      "\n",
      "While the company demonstrates robust financial health and strong long-term stock performance, a significant trend is the decline in gross and net profit margins from FY2022 to the estimated FY2024. Investors are closely monitoring how the company addresses these margin pressures. Determining the \"best\" buying price is subjective and depends on individual investment strategy and risk tolerance, considering both current valuation and future outlook.\n"
     ]
    }
   ],
   "source": [
    "marketAgent = MarketResearchAgent(model=\"gemini-2.5-flash\")\n",
    "response=marketAgent.processUserInput(\"What is the best price to buy Tesla stock right now?\")\n",
    "#tags = marketAgent.getEntities(\"What is the best price to buy Tesla stock right now?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eb24a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketSentimentAgent(Agent):\n",
    "    def __init__(self,model=\"gemini-2.5-flash\"):\n",
    "        name=\"Market News Sentiment Agent\"\n",
    "        model=model\n",
    "        role=\"Market News Sentiment Agent specialized in financial news sentiment analysis\"\n",
    "        system_prompt=f\"\"\"You are a Market Sentiment Agent specialized in financial news sentiment analysis.\n",
    "         Your role is to assist users by analyzing the sentiment of financial news articles and providing insights based on the emotional tone of the content.\n",
    "\n",
    "         Based on the news data retrieved from FinnHub, provide comprehensive sentiment analysis to help users understand market mood and potential impacts on stock performance.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.memory_system=MemorySystem()\n",
    "        super().__init__(name=name,system_prompt=system_prompt,model=model,generate_response=self.generate_response,role=role,agents=None,tools=None,memory_system=self.memory_system,parser=None)\n",
    "        \n",
    "    def generate_response(self, **kwargs): # This is the placeholder of the generative function for the agent, which will receive a variable number of parameters\n",
    "        # print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        input_prompt=kwargs.get(\"prompt\",[])\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=input_prompt,\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "        \n",
    "    def getNewsSummary(self,symbol:str  ) -> str:\n",
    "            prompt=f\"\"\"Provide a comprehensive news summary for the stock symbol: {symbol}.\n",
    "                    Include recent news articles, key events, and any notable trends affecting the stock.\n",
    "                    Use data from FinnHub and other news sources to inform your summary.\n",
    "                    Format the response in a clear and concise manner suitable for a financial report.\"\"\"\n",
    "            insights = self.memory_system.get_news_insights(symbol)\n",
    "            if insights:\n",
    "                print(f\"Using cached insight for symbol {symbol}.\")\n",
    "                return insights[-1]['news_item']\n",
    "            else:\n",
    "                tools_list=[FinancialNews(),RecommendationTrends(),EarningSurprise()]\n",
    "                for tool in tools_list:\n",
    "                    tool_response=tool.invoke(symbol=symbol)\n",
    "                    prompt+=f\"\\nData from {tool.name}: {tool_response}\"\n",
    "                response=self.generate_response(prompt=prompt)\n",
    "                self.memory_system.add_market_news(symbol, response,timestamp=datetime.now().isoformat())\n",
    "            return response\n",
    "        \n",
    "    def  processUserInput(self, user_input: str) -> str:\n",
    "        print(\"-\" * 50)\n",
    "        print(f'{self.name}\" received input: {user_input}')\n",
    "        print(\"-\" * 50)\n",
    "        tags=self.getEntities(user_input=user_input)\n",
    "        if \"symbol\" in tags:\n",
    "            newsSummary=self.getNewsSummary(symbol=tags.get(\"symbol\"))\n",
    "        prompt=f\"\"\"Based on the {newsSummary} Analyze the following user input\n",
    "                and provide a short answer for the user query.\n",
    "                Rules:\n",
    "                - If the user input is related to financial news sentiment, provide insights based on the news summary.\n",
    "                - If the user input is unrelated to financial markets, respond with \"I'm sorry, I can only assist with financial market-related queries.\"\n",
    "                - Keep the response concise and relevant to the user's query.\n",
    "                - Use a professional and informative tone suitable for financial discussions.\n",
    "                - Limit the response to 150 words.\n",
    "\n",
    "                User Input: \"{user_input}\"\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\",\n",
    "        response=self.generate_response(prompt=prompt)\n",
    "        return response\n",
    "    def getEntities(self, user_input: str) -> str:\n",
    "        prompt=f\"\"\"Determine entities the following user input related to financial markets and stock analysis:\n",
    "                if the input contains Apple Inc, return SYMBOL as AAPL\n",
    "                if the input contains Microsoft Corporation, return SYMBOL as MSFT\n",
    "                User Input: \"{user_input}\n",
    "                Extracted Entities:\n",
    "                    <SYMBOL>...</SYMBOL>\n",
    "                    <EXCHANGE>...</EXCHANGE><INDUSTRY>...</INDUSTRY>  \"\"\"\n",
    "        response=self.generate_response(prompt=prompt)\n",
    "        parser=XmlParser()\n",
    "        parsed_response=parser.parseTags(response)\n",
    "        return parsed_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abafc88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached insight for symbol TSLA.\n",
      "Sentiment around Tesla's stock is currently mixed to cautious, despite a net positive analyst outlook with a majority of Buy recommendations. The stock is at a \"make-or-break point\" ahead of its Q3 2025 earnings report, facing concerns over a \"significant front-loading effect\" and a recent rating downgrade.\n",
      "\n",
      "Broader market sentiment is wary due to persistent high EV costs, US-China tensions, and general market volatility. Tesla has also recorded multiple earnings misses in the past three quarters, adding to investor scrutiny. Analysts show increased caution, with a significant portion holding \"Hold\" recommendations, awaiting clearer performance signals.\n"
     ]
    }
   ],
   "source": [
    "newsAgent=MarketSentimentAgent(model=\"gemini-2.5-flash\")\n",
    "response=newsAgent.processUserInput(\"What is the sentiment around Tesla's stock based on the latest news?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d208600-b43d-424f-b54d-7ee6f97c1257",
   "metadata": {},
   "source": [
    "### 5. MAIN ORCHESTRATOR AGENT.\n",
    "#### This is the section where we define the orchestrator agent, which performs the interpretation of the user's prompt, prepares a plan, calls the subagents as needed, and prepares the final answer to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a8fa6bcc-d542-4354-80bc-95ad0e9cf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorAgent(Agent):\n",
    "    def __init__(self, model, agents=None, memory=None, parser=None):\n",
    "        self.agents = agents\n",
    "        #self.agents_description = \"\\n\".join([f\"- {agent.name}: {agent.role}\" for agent in self.agents.items()])\n",
    "        self.agents_description = \"\"\n",
    "        for agent in self.agents:\n",
    "            self.agents_description += f'\\n- {agent.name}: {agent.role}'\n",
    "\n",
    "        system_prompt = f'''\n",
    "          You are the leading AI agent for the following team of agents:\n",
    "            {self.agents_description}\n",
    "\n",
    "            You do not generate a response directly to the user, but instead you'll coordinate the agents team by generating a list of tasks for them to do following the Agent Usage guidelines.\n",
    "            \n",
    "            ### Agent Usage Guidelines:\n",
    "\n",
    "            1. Do not respond to the current input directly. Instead, create a plan to call the research agents in your team to pull the necessary data.\n",
    "            2. Convert that plan into a list of calls for your specialized agents (except for the Writer Agent) in the following format:\n",
    "            <SpecializedAgent>{{\"agentName\": \"MarketResearchAgent\", \"user_input\": \"Your specific query here\"}}</SpecializedAgent>\n",
    "            3. The writer agent will be called separately to finalize the response. Exclude from your thinking process.\n",
    "            \n",
    "            ### Other TAGs you can include in your plan:\n",
    "            -  For thinking, you must wrap your thoughts in <Thought> and </Thought> tags.\n",
    "            -  For final answers, you must wrap your answer in <FinalAnswer> and </FinalAnswer> tags.\n",
    "            -  If you need users to provide more information, you must wrap your request in <RequestMoreInfo> and </RequestMoreInfo> tags.\n",
    "           \n",
    "            ### Instructions for using the tools:\n",
    "            You should only use the information returned by the Agents listed above, never try to get information independently.\n",
    "        '''\n",
    "        system_prompt = system_prompt.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        #We also need to declare a parser\n",
    "        if parser==None:\n",
    "            parser = XmlParser()\n",
    "\n",
    "        super().__init__(\n",
    "            name = \"Orchestrator Agent\", #Name of the Orchestrator class\n",
    "            role = \"Orchestrator Agent that manages tool usage and conversation flow\",\n",
    "            system_prompt = system_prompt,\n",
    "            model = model,\n",
    "            generate_response = self.generate_response,\n",
    "            memory_system = memory,\n",
    "            agents = agents,\n",
    "            parser = parser\n",
    "        )\n",
    "        self.conversation_history = []\n",
    "            # Limit for conversation history\n",
    "        # print(f\"Prompt Template: {self.system_prompt}\")\n",
    "        self.prompt_template = (\n",
    "            f\"{self.system_prompt}\\n\"\n",
    "            \"Conversation history:\\n\"\n",
    "            \"{history}\\n\"\n",
    "            \"Current input: {input}\\n\"\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.initialize_client()\n",
    "\n",
    "    def remember(self, message):\n",
    "        self.conversation_history.append(message)\n",
    "        if len(self.conversation_history) > self.max_history_length:\n",
    "            self.conversation_history.pop(0)\n",
    "    \n",
    "    def generate_response(self, input_prompt):\n",
    "        history_text = \"\\n\".join(self.conversation_history)\n",
    "        #print(\"Conversation History: \")\n",
    "        #print(history_text)\n",
    "        response = \"\"\n",
    "        prompt = self.prompt_template.format(\n",
    "            history=history_text,\n",
    "            input=input_prompt\n",
    "        )\n",
    "        print(f\"Orchestrator Prompt: {prompt}\")\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    #messages=input_prompt,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            self.remember(f\"User: {input_prompt}\")\n",
    "            self.remember(f\"{self.name}: {response}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "        return response\n",
    "    \n",
    "\n",
    "    def get_specialist_opinion(self, agentName, user_input):\n",
    "        '''Agent Orchestrator can call other agents to get their opinion on specific user inputs.'''\n",
    "        MyAgentsTeam = {MyMarketResearcher, MyNewsResearcher, MyWriter}\n",
    "        for agent in self.agents:\n",
    "            if agent.name == agentName:\n",
    "                return agent.processUserInput(user_input)\n",
    "        return f\"Agent {agentName} not found.\"\n",
    "        \n",
    "    \n",
    "    def reAct(self, user_input:str)-> str:\n",
    "        # Here you would implement the logic to parse the response for tool usage\n",
    "        # and handle the tool invocation and results.\n",
    "        parsed_response = \"\"\n",
    "        if self.parser and self.agents:\n",
    "            response = self.generate_response(user_input)\n",
    "            print(\"*\" * 50)\n",
    "            print(f'Raw actions from Orchestrator: {response}')\n",
    "            print(\"*\" * 50)\n",
    "            parsed_response = self.parser.parse(response) ## parsed response is a dict {\"InvokeTool\": \"tool_name\", \"parameters\": {...}} or {\"FinalAnswer\": \"answer\"} or {\"RequestMoreInfo\": \"info\"}\n",
    "            print(\"*\" * 50)\n",
    "            print(f'Actions list from Orchestrator: {parsed_response}')\n",
    "            print(\"*\" * 50)\n",
    "            system_message = f\"System: {response}\"\n",
    "            self.remember(system_message)\n",
    "            self.conversation_history.append(system_message)\n",
    "            '''\n",
    "            parsed_response= {\n",
    "            \"action\": \"InvokeTool\",\n",
    "            \"parameters\": {\n",
    "                \"symbol\": \"AAPL\",\n",
    "                \"step\": \"financials\"\n",
    "                }\n",
    "            }\n",
    "                    '''\n",
    "            action = parsed_response.get(\"action\")\n",
    "            #print(f\"Orchestrator Action: {action}\")\n",
    "            if action == \"SpecializedAgent\":\n",
    "                agent_name = parsed_response[\"parameters\"].get(\"agentName\")\n",
    "                user_input_for_agent = parsed_response[\"parameters\"].get(\"user_input\")\n",
    "                print(\"-\" * 50)\n",
    "                print(f'Orchestrator calling {agent_name} with prompt \"{user_input_for_agent}\"')\n",
    "                print(\"-\" * 50)\n",
    "                agent_response = self.get_specialist_opinion(agent_name, user_input_for_agent)\n",
    "                self.remember(f\"Agent {agent_name} Response: {agent_response}\")\n",
    "                self.conversation_history.append(f\"Agent {agent_name} Response: {agent_response}\") \n",
    "                # Generate a new response based on the agent result\n",
    "                response = self.generate_response(f\"Agent {agent_name} Response: {agent_response}\")\n",
    "                parsed_response = self.parser.parse(response)   \n",
    "            elif action == \"FinalAnswer\" or action == \"RequestMoreInfo\" or action == \"NeedApproval\":\n",
    "                print(f\"Orchestrator Final Response: {response}\")\n",
    "                return parsed_response.get(\"content\")\n",
    "            else:\n",
    "                return \"I'm not sure how to proceed. Could you please clarify?\"\n",
    "            # Handle tool invocation and results based on parsed_response\n",
    "            # This is a placeholder for actual implementation\n",
    "        else:\n",
    "            parsed_response = \"Error: no parser or sub agents found!\"\n",
    "            print('Parser:')\n",
    "            print(self.parser)\n",
    "            print('Agents:')\n",
    "            print(self.agents)\n",
    "            response = parsed_response\n",
    "        #print(f\"Parsed Response: {parsed_response}\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4b7fae3d-16cf-4e77-9fd0-40f33e0ad6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator Prompt: \n",
      "          You are the leading AI agent for the following team of agents:\n",
      "            \n",
      "- Writer: financial content writer\n",
      "- Market Research Agent: Market Research Agent specialized in financial data analysis and market trends\n",
      "- Market News Sentiment Agent: Market News Sentiment Agent specialized in financial news sentiment analysis\n",
      "\n",
      "            You do not generate a response directly to the user, but instead you'll coordinate the agents team by generating a list of tasks for them to do following the Agent Usage guidelines.\n",
      "            \n",
      "            ### Agent Usage Guidelines:\n",
      "\n",
      "            1. Do not respond to the current input directly. Instead, create a plan to call the research agents in your team to pull the necessary data.\n",
      "            2. Convert that plan into a list of calls for your specialized agents (except for the Writer Agent) in the following format:\n",
      "            <SpecializedAgent>{\"agentName\": \"MarketResearchAgent\", \"user_input\": \"Your specific query here\"}</SpecializedAgent>\n",
      "            3. The writer agent will be called separately to finalize the response. Exclude from your thinking process.\n",
      "            \n",
      "            ### Other TAGs you can include in your plan:\n",
      "            -  For thinking, you must wrap your thoughts in <Thought> and </Thought> tags.\n",
      "            -  For final answers, you must wrap your answer in <FinalAnswer> and </FinalAnswer> tags.\n",
      "            -  If you need users to provide more information, you must wrap your request in <RequestMoreInfo> and </RequestMoreInfo> tags.\n",
      "           \n",
      "            ### Instructions for using the tools:\n",
      "            You should only use the information returned by the Agents listed above, never try to get information independently.\n",
      "        \n",
      "Conversation history:\n",
      "\n",
      "Current input: Based on the latest news and stock prices, it is a good time to buy Apple stock today?\n",
      "\n",
      "**************************************************\n",
      "Raw actions from Orchestrator: <Thought>\n",
      "Apple stock is a popular investment choice, and its performance can be influenced by various factors such as market trends, financial news, and stock prices. To determine if it is a good time to buy Apple stock today, we need to analyze the latest news and stock prices related to Apple to assess the current market sentiment and trends.\n",
      "</Thought>\n",
      "\n",
      "<SpecializedAgent>{\"agentName\": \"MarketResearchAgent\", \"user_input\": \"Retrieve the latest stock prices and trends for Apple stock.\"}</SpecializedAgent>\n",
      "<SpecializedAgent>{\"agentName\": \"MarketNewsSentimentAgent\", \"user_input\": \"Gather the sentiment analysis of recent financial news related to Apple.\"}</SpecializedAgent>\n",
      "**************************************************\n",
      "**************************************************\n",
      "Actions list from Orchestrator: {'action': 'SpecializedAgent', 'parameters': {'agentName': 'MarketResearchAgent', 'user_input': 'Retrieve the latest stock prices and trends for Apple stock.'}}\n",
      "**************************************************\n",
      "--------------------------------------------------\n",
      "Orchestrator calling MarketResearchAgent with prompt \"Retrieve the latest stock prices and trends for Apple stock.\"\n",
      "--------------------------------------------------\n",
      "Orchestrator Prompt: \n",
      "          You are the leading AI agent for the following team of agents:\n",
      "            \n",
      "- Writer: financial content writer\n",
      "- Market Research Agent: Market Research Agent specialized in financial data analysis and market trends\n",
      "- Market News Sentiment Agent: Market News Sentiment Agent specialized in financial news sentiment analysis\n",
      "\n",
      "            You do not generate a response directly to the user, but instead you'll coordinate the agents team by generating a list of tasks for them to do following the Agent Usage guidelines.\n",
      "            \n",
      "            ### Agent Usage Guidelines:\n",
      "\n",
      "            1. Do not respond to the current input directly. Instead, create a plan to call the research agents in your team to pull the necessary data.\n",
      "            2. Convert that plan into a list of calls for your specialized agents (except for the Writer Agent) in the following format:\n",
      "            <SpecializedAgent>{\"agentName\": \"MarketResearchAgent\", \"user_input\": \"Your specific query here\"}</SpecializedAgent>\n",
      "            3. The writer agent will be called separately to finalize the response. Exclude from your thinking process.\n",
      "            \n",
      "            ### Other TAGs you can include in your plan:\n",
      "            -  For thinking, you must wrap your thoughts in <Thought> and </Thought> tags.\n",
      "            -  For final answers, you must wrap your answer in <FinalAnswer> and </FinalAnswer> tags.\n",
      "            -  If you need users to provide more information, you must wrap your request in <RequestMoreInfo> and </RequestMoreInfo> tags.\n",
      "           \n",
      "            ### Instructions for using the tools:\n",
      "            You should only use the information returned by the Agents listed above, never try to get information independently.\n",
      "        \n",
      "Conversation history:\n",
      "User: Based on the latest news and stock prices, it is a good time to buy Apple stock today?\n",
      "Orchestrator Agent: ChatCompletion(id='chatcmpl-CSGSgONOgo5QWEDBvZRhQvzuR7YOR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<Thought>\\nApple stock is a popular investment choice, and its performance can be influenced by various factors such as market trends, financial news, and stock prices. To determine if it is a good time to buy Apple stock today, we need to analyze the latest news and stock prices related to Apple to assess the current market sentiment and trends.\\n</Thought>\\n\\n<SpecializedAgent>{\"agentName\": \"MarketResearchAgent\", \"user_input\": \"Retrieve the latest stock prices and trends for Apple stock.\"}</SpecializedAgent>\\n<SpecializedAgent>{\"agentName\": \"MarketNewsSentimentAgent\", \"user_input\": \"Gather the sentiment analysis of recent financial news related to Apple.\"}</SpecializedAgent>', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760852498, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=143, prompt_tokens=366, total_tokens=509, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "System: <Thought>\n",
      "Apple stock is a popular investment choice, and its performance can be influenced by various factors such as market trends, financial news, and stock prices. To determine if it is a good time to buy Apple stock today, we need to analyze the latest news and stock prices related to Apple to assess the current market sentiment and trends.\n",
      "</Thought>\n",
      "\n",
      "<SpecializedAgent>{\"agentName\": \"MarketResearchAgent\", \"user_input\": \"Retrieve the latest stock prices and trends for Apple stock.\"}</SpecializedAgent>\n",
      "<SpecializedAgent>{\"agentName\": \"MarketNewsSentimentAgent\", \"user_input\": \"Gather the sentiment analysis of recent financial news related to Apple.\"}</SpecializedAgent>\n",
      "System: <Thought>\n",
      "Apple stock is a popular investment choice, and its performance can be influenced by various factors such as market trends, financial news, and stock prices. To determine if it is a good time to buy Apple stock today, we need to analyze the latest news and stock prices related to Apple to assess the current market sentiment and trends.\n",
      "</Thought>\n",
      "\n",
      "<SpecializedAgent>{\"agentName\": \"MarketResearchAgent\", \"user_input\": \"Retrieve the latest stock prices and trends for Apple stock.\"}</SpecializedAgent>\n",
      "<SpecializedAgent>{\"agentName\": \"MarketNewsSentimentAgent\", \"user_input\": \"Gather the sentiment analysis of recent financial news related to Apple.\"}</SpecializedAgent>\n",
      "Agent MarketResearchAgent Response: Agent MarketResearchAgent not found.\n",
      "Agent MarketResearchAgent Response: Agent MarketResearchAgent not found.\n",
      "Current input: Agent MarketResearchAgent Response: Agent MarketResearchAgent not found.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<RequestMoreInfo>Please provide more details or clarify the request regarding the Market Research Agent. Thank you!</RequestMoreInfo>'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use \"gpt-3.5-turbo\" or \"gemini-2.5-flash\", #Uncomment if using Google GenAI\n",
    "\n",
    "#Let's declare our sub agent instances first.\n",
    "MyMarketResearcher = MarketResearchAgent(model=\"gemini-2.5-flash\")\n",
    "MyNewsResearcher = MarketSentimentAgent(model=\"gemini-2.5-flash\")\n",
    "MyWriter = WriterAgent(model=\"gemini-2.5-flash\")\n",
    "\n",
    "#We put all of our sub agents together as a list of objects\n",
    "MyAgentsTeam = {MyMarketResearcher, MyNewsResearcher, MyWriter}\n",
    "\n",
    "#Now, we declare our main orchestrator agent instance.\n",
    "MyOrchestrator = OrchestratorAgent(model=\"gpt-3.5-turbo\", agents=MyAgentsTeam)\n",
    "sample_prompt = \"Based on the latest news and stock prices, it is a good time to buy Apple stock today?\"\n",
    "\n",
    "MyOrchestrator.reAct(sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea2e1e-d811-44cb-83eb-1afd0d66f720",
   "metadata": {},
   "source": [
    "### 6. DEMO.\n",
    "#### This is the final section, which contains the implementation of the entire system using all elements above for a quick demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6e69f-4a54-4dc9-ade0-1c38031f6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_tools = [tools.YahooFinance(), tools.FMP(), tools.FinnHub()]\n",
    "toolsList = [ tool.to_dict() for tool in registered_tools ]\n",
    "executionMap= { tool.name: tool for tool in registered_tools }\n",
    "\n",
    "executionMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbc0c6-0f5d-477f-9e3e-925cef221d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.parser as parser\n",
    "parser = parser.XmlParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4fff4-e92e-4b1f-a976-497ffa407111",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = systemPrompt\n",
    "userText =\"\"\n",
    "continueFlag = False\n",
    "exitFlag = \"\"\n",
    "debug = False\n",
    "while userText.lower() != \"exit\" or exitFlag.lower() != \"y\":\n",
    "    if not continueFlag:\n",
    "        userText = input(\"User: \")\n",
    "        if userText.lower() == \"exit\":\n",
    "            break\n",
    "        prompt += \"\\n User:\"+ userText\n",
    "        continueFlag = False\n",
    "    if debug:\n",
    "        print('Prompt: ')\n",
    "        print(prompt)\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\", contents=prompt\n",
    "    )\n",
    "    if debug:\n",
    "        print('Model response: ')\n",
    "        print(response.text)\n",
    "    prompt = prompt + response.text\n",
    "    actions = parser.parse_all(response.text.replace('\\n', ' '))\n",
    "    if debug:\n",
    "        print('Actions: ')\n",
    "        print(actions)\n",
    "    if not actions:\n",
    "        continue\n",
    "    if len(actions) >= 1:\n",
    "        actions = { action['action']: action for action in actions if action.get(\"action\",\"\") != \"Final Answer\" }\n",
    "        if \"NeedApproval\" in actions:\n",
    "            actions.pop(\"NeedApproval\")\n",
    "            if \"InvokeTool\" in actions:\n",
    "                #result=input(\"Need to Call \" + actions[\"InvokeTool\"].get(\"name\",\"\") + \" Y/y to continue...)\")\n",
    "                result=input(\"Need to Call \" + actions[\"InvokeTool\"]['parameters']['name'] + \". Type Y/y to continue...)\")\n",
    "            else:\n",
    "                result=input(\"Need User Approval Y/y to continue...)\")\n",
    "            if result.lower() != \"y\":\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "            actions[\"NeedApproval\"] = {\"action\":\"NeedApproval\", \"content\":\"User approved to continue.\"}\n",
    "            prompt += \"\\n User:\"+ actions[\"NeedApproval\"].get(\"content\",\"\")\n",
    "        if 'InvokeTool' in actions:\n",
    "            action = actions[\"InvokeTool\"]\n",
    "            tools_name = action[\"parameters\"][\"name\"]\n",
    "            tool_params = action[\"parameters\"][\"api\"]\n",
    "            result = executionMap[tools_name].invoke(**json.loads(tool_params))\n",
    "            actions[\"Tool result\"] = {\"action\":\"Tool result\", \"content\":result}\n",
    "            prompt += \"\\n User:\"+ str(result)\n",
    "            continueFlag = True\n",
    "        if \"FinalAnswer\" in actions:\n",
    "            content = actions[\"FinalAnswer\"]['parameters']['content']\n",
    "            print(\"Final Answer: \"+ str(content))\n",
    "            exitFlag = input(\"Do you want to exit? Type Y/y to exit...\")\n",
    "            if exitFlag.lower() == \"y\":\n",
    "                print('Thanks for chatting! Goodbye!')\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a113ed-09dc-4d20-ba76-baa879315c15",
   "metadata": {},
   "source": [
    "## CONCLUSION.\n",
    "#### Enter our conclusions here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
